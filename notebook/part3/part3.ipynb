{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHItaUbRB74u"
   },
   "source": [
    "![](../../jpg/stock_small.jpg)\n",
    "\n",
    "The third notebook builds on what you learned in part 1 and 2. It implements an end-to-end [ML use-case](#ML-integration-Using-Weka-Machine-Learning) including data ingestion, ETL/ELT transformations, model training, model scoring, and result visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To get started you need a Snowflake account and read/write access to a database. If you do not have a Snowflake account, you can sign up for a [free trial](https://signup.snowflake.com/). It doesn't even require a credit card.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we have to set up the environment for our notebook. The instructions for setting up the environment are in the Snowpark documentation in section [configuring-the-jupyter-notebook-for-snowpark](https://docs.snowflake.com/en/developer-guide/snowpark/quickstart-jupyter.html#configuring-the-jupyter-notebook-for-snowpark).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 1\n",
    "\n",
    "Configure the notebook to use a Maven repository for a library that Snowpark depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val osgeoRepo = coursierapi.MavenRepository.of(\"https://repo.osgeo.org/repository/release\")\n",
    "interp.repositories() ++= Seq(osgeoRepo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 2\n",
    "\n",
    "Create a directory (if it doesn't exist) for temporary files created by the [REPL](https://ammonite.io/#Ammonite-REPL) environment. To avoid any side effects from previous runs, we also delete any files in that directory.\n",
    "\n",
    "**Note: Make sure that you have the operating system permissions to create a directory in that location.**\n",
    "\n",
    "**Note: If you are using multiple notebooks, youâ€™ll need to create and configure a separate REPL class directory for each notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ammonite.ops._\n",
    "import ammonite.ops.ImplicitWd._\n",
    "\n",
    "// This folder is used to store generated REPL classes, which will later be used in UDFs.\n",
    "// Please provide an empty folder path. This is essential for Snowpark UDFs to work\n",
    "val replClassPath = pwd+\"/repl_classes\"\n",
    "\n",
    "// Delete any old files in the directory.\n",
    "import sys.process._\n",
    "s\"rm -rf $replClassPath\" !\n",
    "\n",
    "// Create the REPL class folder.\n",
    "import sys.process._\n",
    "s\"mkdir -p $replClassPath\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 3\n",
    "\n",
    "Configure the compiler for the Scala REPL. This does the following:\n",
    "- Configures the compiler to generate classes for the REPL in the directory that you created earlier.\n",
    "- Configures the compiler to wrap code entered in the REPL in classes, rather than in objects.\n",
    "- Adds the directory that you created earlier as a dependency of the REPL interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "// Generate all REPL classes in REPL class folder\n",
    "interp.configureCompiler(_.settings.outputDirs.setSingleOutput(replClassPath))\n",
    "interp.configureCompiler(_.settings.Yreplclassbased.value = true)\n",
    "interp.load.cp(os.Path(replClassPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 4\n",
    "Import the Snowpark library from Maven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`com.snowflake:snowpark:0.8.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To create a session, we need to authenticate ourselves to the Snowflake instance. Though it might be tempting to just override the authentication variables below with hard coded values, it's not considered best practice to do so. If you  share your version of the notebook, you might disclose your credentials by mistake to the recipient. Even worse, if you upload your notebook to a public code repository, you might advertise your credentials to the whole world. To prevent that, you should keep your credentials in an external file (like we are doing here).\n",
    "\n",
    "Then, update your credentials in that file and they will be saved on your local machine. Even better would be to switch from user/password authentication to [private key authentication](https://docs.snowflake.com/en/user-guide/key-pair-auth.html). \n",
    "\n",
    "Copy the credentials template file creds/template_credentials.txt to creds/credentials.txt and update the file with your credentials. Put your key files into the same directory or update the location in your credentials file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "kGGUvHHYjiGj"
   },
   "outputs": [],
   "source": [
    "import com.snowflake.snowpark._\n",
    "import com.snowflake.snowpark.functions._\n",
    "\n",
    "val session = Session.builder.configFile(\"creds/credentials.txt\").create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 5\n",
    "Add the Ammonite kernel classes as dependencies for your UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def addClass(session: Session, className: String): String = {\n",
    "  var cls1 = Class.forName(className)\n",
    "  val resourceName = \"/\" + cls1.getName().replace(\".\", \"/\") + \".class\"\n",
    "  val url = cls1.getResource(resourceName)\n",
    "  val path = url.getPath().split(\":\").last.split(\"!\").head\n",
    "  session.addDependency(path)\n",
    "  path\n",
    "}\n",
    "addClass(session, \"ammonite.repl.ReplBridge$\")\n",
    "addClass(session, \"ammonite.interp.api.APIHolder\")\n",
    "addClass(session, \"pprint.TPrintColors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 6\n",
    "\n",
    "For this exercise we need these additional libraries.\n",
    "- [plotl-scala (Plotly for Scala)](https://github.com/alexarchambault/plotly-scala)\n",
    "- [Weka Machine Learning)](https://www.cs.waikato.ac.nz/ml/weka/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`nz.ac.waikato.cms.weka:weka-stable:3.8.5`\n",
    "import $ivy.`org.plotly-scala::plotly-almond:0.8.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ML integration Using Weka Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following example is based on the [Kaggle Customer Churn Dataset](https://www.kaggle.com/blastchar/telco-customer-churn). The dataset provides information about Telco customers and the goal is to predict customer churn, i.e. is a customer at risk of cancelling their service. To predict customer churn, we will use the [Weka Machine Learning Library](https://waikato.github.io/weka-wiki/).  \n",
    "\n",
    "The example is structured in 6 steps.\n",
    "\n",
    "- [Define configuration parameters](#Configuration)\n",
    "- [Data preparation](#Data-Preparation)\n",
    "- [Build user defined functions to top of the Weka API](#User-Defined-Functions)\n",
    "- [Train a model using a classification algorithm (locally)](#Model-Training): \n",
    "- [Bulk scoring on dataset in Snowflake (remote)](#Scoring)\n",
    "- [Visualize result (confusion matrix)](#Result-Visualization)\n",
    "\n",
    "For ease of use, the dataset has been included in this repo but could be downloaded from Kaggle as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The code in this example is fully configurable. \n",
    "\n",
    "- Tables: Import/Clean/Train/Test/OutTraining; Tables are fully qualified \n",
    "- Exclusion List : Columns to be excluded from the prediction/inference step\n",
    "- Key Column: Column name uniquely identifying a a row to be scored\n",
    "- Predict Column: Column to be predicted\n",
    "- Model additional Parameters: Name of the Attribute Relation File Format (ARFF), Model Type (Algorithm), Number of Splits, Name & Location of the file holding the serialized model\n",
    "\n",
    "This example is pre-configured but you could bring your own data, change the parameters listed above, and start building your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "// Parameters for run:\n",
    "val trainDb=\"DEMO_DB\"\n",
    "val trainSchema=\"SANDBOX_RFEHRMANN\"\n",
    "val importTable = List(trainDb,trainSchema,\"TELCO_IMPORT\")\n",
    "val cleanTable = List(trainDb,trainSchema,\"TELCO_CLEAN\")\n",
    "val trainTable = List(trainDb,trainSchema,\"TELCO_TRAIN\")\n",
    "val testTable = List(trainDb,trainSchema,\"TELCO_TEST\")\n",
    "val outTable = List(trainDb,trainSchema,\"TELCO_OUT\" )\n",
    "val trainExcludeCols = List(\"CUSTOMERID\")\n",
    "val scoreKeyColumn = \"CUSTOMERID\"\n",
    "val predictColumn = \"CHURN\"\n",
    "val modelTypeName = \"trees.J48\"\n",
    "val modelARFFName = \"telco_churn\"\n",
    "val modelTrainingSplits = 2             \n",
    "val persistedModelFilename = \"TelcoModelOut\"\n",
    "val persistedModelPath = \"extraJars\"\n",
    "val persistedModel = persistedModelPath + \"/\" + persistedModelFilename\n",
    "/////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Data prep can be a very time consuming process when building an ML pipeline. And this is not just in terms of time spent for for the actual processing but also in terms of developer time spent on building the process. In the following cells, you can see how easy it is to perform the necessary steps of loading data, cleansing the data, and  splitting the data into training and test partitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The data import steps assumes that the dataset is stored in CSV format and that it is stored in the directory data/telco. Change the location if your files are stored in a different location. The datafile, in this case *WA_Fn-UseC_-Telco-Customer-Churn.csv.gz*, is uploaded into a Snowflake user stage via a *put* command. Importing the file into Snowflake is as easy as reading the file into a Snowflake DataFrame and writing the DatasFrame into a Snowflake table. All we need is the file structure, i.e. table schema, and the file location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import com.snowflake.snowpark.types._\n",
    "\n",
    "session.sql(\"\"\"\n",
    "     put file://data/telco/WA_Fn-UseC_-Telco-Customer-Churn.csv.gz @~/upload/ overwrite=true\n",
    "  \"\"\").show()\n",
    "\n",
    "val TelcoSchema=StructType(\n",
    "  Seq(\n",
    "      StructField(\"CUSTOMERID\",StringType,true),\n",
    "      StructField(\"GENDER\",StringType,true),\n",
    "      StructField(\"SENIORCITIZEN\",StringType,true),\n",
    "      StructField(\"PARTNER\",StringType,true),\n",
    "      StructField(\"DEPENDENTS\",StringType,true),\n",
    "      StructField(\"TENURE\",StringType,true),\n",
    "      StructField(\"PHONESERVICE\",StringType,true),\n",
    "      StructField(\"MULTIPLELINES\",StringType,true),\n",
    "      StructField(\"INTERNETSERVICE\",StringType,true),\n",
    "      StructField(\"ONLINESECURITY\",StringType,true),\n",
    "      StructField(\"ONLINEBACKUP\",StringType,true),\n",
    "      StructField(\"DEVICEPROTECTION\",StringType,true),\n",
    "      StructField(\"TECHSUPPORT\",StringType,true),\n",
    "      StructField(\"STREAMINGTV\",StringType,true),\n",
    "      StructField(\"STREAMINGMOVIES\",StringType,true),\n",
    "      StructField(\"CONTRACT\",StringType,true),\n",
    "      StructField(\"PAPERLESSBILLING\",StringType,true),\n",
    "      StructField(\"PAYMENTMETHOD\",StringType,true),\n",
    "      StructField(\"MONTHLYCHARGES\",StringType,true),\n",
    "      StructField(\"TOTALCHARGES\",StringType,true),\n",
    "      StructField(\"CHURN\",StringType,true)\n",
    "  )\n",
    ")\n",
    "val importDf = session\n",
    "   .read\n",
    "   .option(\"skip_header\", 1)\n",
    "   .schema(TelcoSchema)\n",
    "   .csv(\"@~/upload/WA_Fn-UseC_-Telco-Customer-Churn.csv.gz\")\n",
    "\n",
    "importDf\n",
    "   .write\n",
    "   .mode(SaveMode.Overwrite)\n",
    "   .saveAsTable(importTable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Transformations and Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Transformation and feature generation usually require many more steps and effort. In this particular case, the data is already pretty clean and one-hot-encoding is automatically done by Weka. Weka takes all string variables and automatically one-hot-encodes all values. The only requirement is that string values can not include spaces. Numbers (currently of datatype string in the import table) need to be cast to number types (i.e. double, integer, ...), otherwise Weka would create one-hot-encoded features. \n",
    "\n",
    "The cell below does exactly that. It replaces spaces, and casts numbers to *Integer* and *Double* types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val cleanDf=session.table(importTable)\n",
    "   .select(\n",
    "         replace(col(\"CUSTOMERID\"),lit(' '),lit('-')).name(\"CUSTOMERID\")\n",
    "         ,replace(col(\"GENDER\"),lit(' '),lit('-')).name(\"GENDER\")\n",
    "         ,col(\"SENIORCITIZEN\").cast(IntegerType).name(\"SENIORCITIZEN\")\n",
    "         ,replace(col(\"PARTNER\"),lit(' '),lit('-')).name(\"PARTNER\")\n",
    "         ,replace(col(\"DEPENDENTS\"),lit(' '),lit('-')).name(\"DEPENDENTS\")\n",
    "         ,col(\"TENURE\").cast(IntegerType).name(\"TENURE\")\n",
    "         ,replace(col(\"PHONESERVICE\"),lit(' '),lit('-')).name(\"PHONESERVICE\")\n",
    "         ,replace(col(\"MULTIPLELINES\"),lit(' '),lit('-')).name(\"MULTIPLELINES\")\n",
    "         ,replace(col(\"INTERNETSERVICE\"),lit(' '),lit('-')).name(\"INTERNETSERVICE\")\n",
    "         ,replace(col(\"ONLINESECURITY\"),lit(' '),lit('-')).name(\"ONLINESECURITY\")\n",
    "         ,replace(col(\"ONLINEBACKUP\"),lit(' '),lit('-')).name(\"ONLINEBACKUP\")\n",
    "         ,replace(col(\"DEVICEPROTECTION\"),lit(' '),lit('-')).name(\"DEVICEPROTECTION\")\n",
    "         ,replace(col(\"TECHSUPPORT\"),lit(' '),lit('-')).name(\"TECHSUPPORT\")\n",
    "         ,replace(col(\"STREAMINGTV\"),lit(' '),lit('-')).name(\"STREAMINGTV\")\n",
    "         ,replace(col(\"STREAMINGMOVIES\"),lit(' '),lit('-')).name(\"STREAMINGMOVIES\")\n",
    "         ,replace(col(\"CONTRACT\"),lit(' '),lit('-')).name(\"CONTRACT\")\n",
    "         ,replace(col(\"PAPERLESSBILLING\"),lit(' '),lit('-')).name(\"PAPERLESSBILLING\")\n",
    "         ,replace(col(\"PAYMENTMETHOD\"),lit(' '),lit('-')).name(\"PAYMENTMETHOD\")           \n",
    "         ,col(\"MONTHLYCHARGES\").cast(DoubleType).name(\"MONTHLYCHARGES\")\n",
    "         ,replace(col(\"TOTALCHARGES\"),lit(' '),lit('0')).cast(DoubleType).name(\"TOTALCHARGES\")           \n",
    "         ,replace(col(\"CHURN\"),lit(' '),lit('-')).name(\"CHURN\") )\n",
    "\n",
    "cleanDf\n",
    "  .write\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .saveAsTable(cleanTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For training and testing purposes we split the dataset into a: \n",
    "\n",
    "- training dataset (66% of the data)\n",
    "- test dataset (34% of the data)\n",
    "\n",
    "and write both datasets back into Snowflake as tables. \n",
    "\n",
    "For the purpose of this example, we use a random split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val weights:Array[Double]=Array(66.0,34.0)\n",
    "val allDataSplitDf=session.table(cleanTable)\n",
    "     .randomSplit(weights)\n",
    "\n",
    "allDataSplitDf(0)\n",
    "   .write\n",
    "   .mode(SaveMode.Overwrite)\n",
    "   .saveAsTable(trainTable)\n",
    "\n",
    "allDataSplitDf(1)\n",
    "   .write\n",
    "   .mode(SaveMode.Overwrite)\n",
    "   .saveAsTable(testTable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## User Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In comparison to Part2 of this series, the code for these UDFs is much more advanced. Code is split into two major objects. The code that executes on the client (local) machine, and the code that executes on the Snowflake (remote) side.\n",
    "From a high level perspective we need to take the training data, serialize it (getArffRows), pass it as a parameter to the training step (trainAndSaveModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remote Side Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "On the Snowflake (remote) side we only score the model. Model training is done on the client (docker container running the notebook server). Therefore, the remote class is very straightforward. There is just one important detail. The two files needed for scoring, i.e. the model file and the ARFF header file, should be loaded only once during the initialization of the WekaRemoteHelper class. You can find additional detail on this pattern in the Snowflake documentation for writing [UDF initialization code](https://docs.snowflake.com/en/developer-guide/snowpark/creating-udfs.html#writing-initialization-code-for-a-udf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class WekaRemoteHelper (persistedModel:String) extends Serializable {\n",
    "    import java.io.BufferedInputStream\n",
    "    import java.io.ByteArrayInputStream\n",
    "    import java.io.ObjectInputStream\n",
    "    import java.io.BufferedReader\n",
    "    import java.io.StringReader\n",
    "\n",
    "    import weka.classifiers.Classifier\n",
    "    import weka.core.Instances\n",
    "\n",
    "    private lazy val arffHdr:String = {\n",
    "        val persistedModelResource = \"/\" + persistedModel + \".arff\"\n",
    "        val sourceArff = new BufferedInputStream(getClass.getResourceAsStream(persistedModelResource))\n",
    "        Stream.continually(sourceArff.read).takeWhile(-1 !=).map(_.toChar).mkString\n",
    "    }\n",
    "\n",
    "    private lazy val cls: Classifier = {\n",
    "        val sourceMdlResource =  \"/\" + persistedModel + \".mdl\"\n",
    "        val sourceMdl = new BufferedInputStream(getClass.getResourceAsStream(sourceMdlResource))\n",
    "        val trainedModel: Array[Byte] = Stream.continually(sourceMdl.read).takeWhile(-1 !=).map(_.toByte).toArray\n",
    "        new ObjectInputStream(new ByteArrayInputStream(trainedModel))\n",
    "               .readObject().asInstanceOf[Classifier]\n",
    "    }\n",
    "\n",
    "    def scoreRow (rowArff: String) : (String)  = {\n",
    "        val scoreThis = new Instances(new BufferedReader(new StringReader(arffHdr + rowArff)))\n",
    "        scoreThis.setClassIndex(scoreThis.numAttributes() - 1)\n",
    "        // Only a single row (instance (0)) to score per UDF call\n",
    "        val res = cls.classifyInstance(scoreThis.instance(0))\n",
    "        scoreThis.classAttribute().value(res.asInstanceOf[Int])\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Client Side Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Weka client side helper Object includes:\n",
    "- model building functions and sub-functions (trainAndSaveModel, calculateAccuracy, crossValidationSplit, classify, saveModel, loadModel)\n",
    "- ARFF serialization functions (getArffHeader, getArffRows)\n",
    "- row serialization function (concatAllCol)\n",
    "- the model building steps and serialization functions for the ARFF headerand dat rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object WekaClientHelper {\n",
    "    import java.io.BufferedOutputStream\n",
    "    import java.io.FileOutputStream\n",
    "    import java.io.ByteArrayOutputStream\n",
    "    import java.io.ObjectInputStream\n",
    "    import java.io.ByteArrayInputStream\n",
    "    import java.io.StringReader\n",
    "    import java.io.BufferedReader\n",
    "    import java.io.BufferedInputStream\n",
    "    import java.io.BufferedInputStream\n",
    "    import java.io.FileInputStream\n",
    "\n",
    "    import java.util.Base64\n",
    "    import java.util.ArrayList\n",
    "\n",
    "    import weka.classifiers.Classifier\n",
    "    import weka.classifiers.Evaluation\n",
    "    import weka.classifiers.evaluation.NominalPrediction\n",
    "    import weka.classifiers.evaluation.NumericPrediction\n",
    "    import weka.classifiers.functions.GaussianProcesses\n",
    "    import weka.classifiers.rules.DecisionTable\n",
    "    import weka.classifiers.rules.PART\n",
    "    import weka.classifiers.trees.DecisionStump\n",
    "    import weka.classifiers.functions.Logistic\n",
    "    import weka.classifiers.bayes.NaiveBayesUpdateable\n",
    "    import weka.classifiers.trees.J48\n",
    "    import weka.classifiers.trees.RandomForest\n",
    "    import weka.classifiers.evaluation.Prediction\n",
    "    import weka.classifiers.functions.LinearRegression\n",
    "\n",
    "    import weka.core.DenseInstance\n",
    "    import weka.core.Instance\n",
    "    import weka.core.Instances\n",
    "\n",
    "    import java.security.MessageDigest\n",
    "\n",
    "    import com.snowflake.snowpark.SaveMode.Overwrite\n",
    "\n",
    "    def md5(s: String) = {\n",
    "      MessageDigest.getInstance(\"MD5\").digest(s.getBytes)\n",
    "    }\n",
    "\n",
    "    def concatAllCol(df : DataFrame, trainExcludeCols : List[String] ) : Column = {\n",
    "        val colList = df.drop(trainExcludeCols).schema.map(c => c.dataType.typeName match {\n",
    "              case \"Long\"   => coalesce(callBuiltin(\"to_varchar\", df.col(c.name)), lit('?'))\n",
    "              case \"Double\" => coalesce(callBuiltin(\"to_varchar\", df.col(c.name)), lit('?'))\n",
    "              case \"Date\"   => coalesce(callBuiltin(\"to_varchar\", df.col(c.name)), lit('?'))\n",
    "              case \"String\" => coalesce(df.col(c.name),lit('?'))\n",
    "              case _        => throw new UnsupportedOperationException(\"Column Type Not Handled Yet\")         \n",
    "          } \n",
    "        )\n",
    "        concat_ws(lit(\",\"), colList:_*)\n",
    "    }\n",
    "\n",
    "    def getArffHeader(df : DataFrame, resArr : Array[Seq[Any]], label : String) : String = {\n",
    "        val header = (0 to df.schema.length - 1)\n",
    "          .map(col => \"@attribute \"+df.schema.names(col) + {\n",
    "              // Construct dictionary of categorical values for all STRING data, or tag NUMERIC data, in header\n",
    "              df.schema.fields(col).dataType.typeName match {\n",
    "                case \"Long\"   => \" NUMERIC\"\n",
    "                case \"Double\" => \" NUMERIC\"\n",
    "                case \"Date\"   => \" DATE yyyy-MM-dd\"\n",
    "                case \"String\" => resArr.map( r => r(col)).distinct.mkString(\" { \", \",\", \" }\")\n",
    "                case _        => throw new UnsupportedOperationException(\"Column Type Not Handled Yet\")\n",
    "              }\n",
    "          })\n",
    "          .mkString(\"\\n\")\n",
    "\n",
    "        // return final header String\n",
    "        \"@relation \" + label + \"\\n\" + header + \"\\n@data\\n\"\n",
    "    }\n",
    "\n",
    "    def getArffRows(resArr : Array[Seq[Any]]) : String = {\n",
    "      // Construct comma-delimited ARFF body of rows\n",
    "      // Render any NULLs as ? for ARFF\n",
    "      resArr.map(r => r.map(\n",
    "        c => if ( c == null ) \"?\" else c.toString())\n",
    "        .mkString(\",\"))\n",
    "        .mkString(\"\\n\")\n",
    "    }      \n",
    "   \n",
    "    def classify(model : Classifier, trainingSet : Instances, testingSet : Instances)\n",
    "        : (Evaluation, Array[Byte]) = {\n",
    "            \n",
    "        val evaluation = new Evaluation(trainingSet)\n",
    "        model.buildClassifier(trainingSet)\n",
    "        evaluation.evaluateModel(model, testingSet)\n",
    "        val os = new ByteArrayOutputStream()\n",
    "        weka.core.SerializationHelper.write(os, model)\n",
    "        val bArrayOut = os.toByteArray()\n",
    "        (evaluation, bArrayOut)\n",
    "    }\n",
    "\n",
    "\n",
    "    def crossValidationSplit(data : Instances, numberOfFolds : Int) : Array[Array[Instances]] = {\n",
    "        val split = Array.ofDim[Instances](2, numberOfFolds)\n",
    "        for (i <- (0 to numberOfFolds-1)) {\n",
    "            split(0)(i) = data.trainCV(numberOfFolds, i)\n",
    "            split(1)(i) = data.testCV(numberOfFolds, i)\n",
    "        }\n",
    "        split\n",
    "    }\n",
    "\n",
    "    def calculateAccuracy(predictions : ArrayList[Prediction]) : Double = {\n",
    "        var correct : Int = 1\n",
    "        predictions.forEach( p => {\n",
    "            val np = p.asInstanceOf[NominalPrediction]\n",
    "            if (np.predicted() == np.actual()) correct += 1\n",
    "        })\n",
    "        100.0 * correct / predictions.size()\n",
    "    } \n",
    "\n",
    "    def trainAndTestWithData(modelType : String, inData : String, nSplits : Int) \n",
    "        : (String, Array[Byte]) = {\n",
    "\n",
    "        //val data = new Instances(new BufferedReader(new StringReader(inData)))\n",
    "        val inputString = new StringReader(inData)\n",
    "        val dataFile = new BufferedReader(inputString)\n",
    "        val data = new Instances(dataFile)\n",
    "\n",
    "        data.setClassIndex(data.numAttributes() - 1)\n",
    "\n",
    "        // Do N-split cross validation\n",
    "        val split = crossValidationSplit(data, nSplits)\n",
    "        // Separate split into training and testing arrays\n",
    "        val (trainingSplits, testingSplits) = (split(0), split(1))\n",
    "\n",
    "        val model  = modelType match {\n",
    "            case \"trees.J48\" => new J48()\n",
    "            case \"trees.RandomForest\" => new RandomForest()\n",
    "            case \"functions.Logistic\" =>  new Logistic()\n",
    "            case \"bayes.NaiveBayesUpdateable\" =>  new NaiveBayesUpdateable()\n",
    "            case \"functions.LinearRegression\" => new LinearRegression();\n",
    "        }\n",
    "\n",
    "        var predictions = new ArrayList[Prediction]()\n",
    "        var finalModel = new Array[Byte](0)\n",
    "\n",
    "        for (i <- (0 to trainingSplits.length-1)) {\n",
    "            val (eval, modelBytes) = classify(model, trainingSplits(i), testingSplits(i))\n",
    "            predictions.addAll(eval.predictions())\n",
    "            finalModel = modelBytes\n",
    "            // Uncomment to see the summary for each training-testing pair.\n",
    "            //System.out.println(models[j].toString());\n",
    "        }\n",
    "\n",
    "        val accuracy = calculateAccuracy(predictions)\n",
    "\n",
    "        println(\"Accuracy of \" + model.getClass().getSimpleName() + \": \"\n",
    "              + f\"$accuracy%.2f%%\"\n",
    "              //+ String.format(\"%.2f%%\", accuracy)\n",
    "              + \"\\n---------------------------------\")\n",
    "        val outval = \"InputLength=\"+inData.length() + \", Accuracy of \" + model.getClass().getSimpleName() + \"= \" + f\"$accuracy%.2f%%\" + \" \"\n",
    "        val json_1 = \"{ \\\"model_stats\\\": \\\"\" + outval + \"\\\"}\"\n",
    "\n",
    "        (json_1, finalModel)\n",
    "    }\n",
    "    \n",
    "    def trainAndSaveModel ( trainingSplits : Int, arffRelationName : String, modelTypeName : String, trainExcludeCols : List [String], trainTable : List[String], persistedModel : String ) : (String,  String) = {\n",
    "      // Retrieve all columns from the table except the row-key \n",
    "      val df = session.table(trainTable).drop(trainExcludeCols)\n",
    "      // get results as a scala array of sequences\n",
    "      val resArr = df.collect().map( r => r.toSeq)\n",
    "      val arffHdr = getArffHeader(df, resArr, arffRelationName)\n",
    "      val (resultStr, modelBinary) = trainAndTestWithData(modelTypeName, arffHdr + getArffRows(resArr),  trainingSplits)\n",
    "      // Serialize the model into the resources folder\n",
    "      saveModel(persistedModel, modelBinary, arffHdr)\n",
    "      (resultStr,  arffHdr)\n",
    "    } \n",
    "\n",
    "    def saveModel(persistedModel : String, modelBinary : Array[Byte], arffHdr : String) : Unit = {\n",
    "      val targetMdl = new BufferedOutputStream( new FileOutputStream(persistedModel +\".mdl\") )\n",
    "      try modelBinary.foreach( targetMdl.write(_) ) finally targetMdl.close\n",
    "      val targetArff = new BufferedOutputStream( new FileOutputStream(persistedModel +\".arff\") )\n",
    "      try arffHdr.foreach( targetArff.write(_) ) finally targetArff.close\n",
    "    }\n",
    "\n",
    "    def loadModel(persistedModelFileName : String ) : (Array[Byte], String) = {\n",
    "      val sourceMdl = new BufferedInputStream( new FileInputStream(persistedModelFileName+\".mdl\") )\n",
    "      val mdl = Stream.continually(sourceMdl.read).takeWhile(-1 !=).map(_.toByte).toArray\n",
    "      val sourceArff = new BufferedInputStream( new FileInputStream(persistedModelFileName+\".arff\") )\n",
    "      val arff = Stream.continually(sourceArff.read).takeWhile(-1 !=).map(_.toChar).mkString\n",
    "\n",
    "      (mdl, arff)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The model training method gets these parameters.\n",
    "\n",
    "- Number of training data splits\n",
    "- Name of the ARFF Relation (only visible in the ARFF Header)\n",
    "- List of columns to exclude from training\n",
    "- Name of the training table\n",
    "- Path name of the serialized model file\n",
    "\n",
    "It returns:\n",
    "\n",
    "- Model stats like file length and accuracy\n",
    "- ARFF Header\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import ammonite.ops._\n",
    "import ammonite.ops.ImplicitWd._\n",
    "\n",
    "// This folder is used to store generated repl classes, which will later be used in UDF.\n",
    "// Please provide an empty folder path.This is essential for Snowpark UDF to work\n",
    "val extraJars = pwd+\"/\"+persistedModelPath\n",
    "// Create the repl class folder\n",
    "import sys.process._\n",
    "s\"mkdir -p $extraJars\" !\n",
    "\n",
    "import sys.process._\n",
    "s\"rm -rf $extraJars/*\" !\n",
    "\n",
    "import ammonite.ops._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "val (trainingResults, myArffHdr) = WekaClientHelper\n",
    "   .trainAndSaveModel(modelTrainingSplits,modelARFFName, modelTypeName, trainExcludeCols , trainTable, persistedModel )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After completing the training step, the model has been stored on local storage and it's ready for bundling into a JAR file. In addition to bundling the files into a JAR file, the following step also creates a dependency to the newly created JAR file so Snowpark will automatically upload the JAR file to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%%('bash, \"-c\", \"jar cf $PPATH/$NAME.jar $PPATH/$NAME.arff $PPATH/$NAME.mdl\", PPATH=persistedModelPath, NAME=persistedModelFilename)\n",
    "%%('bash, \"-c\", \"jar tf $PPATH/$NAME.jar \", PPATH=persistedModelPath, NAME=persistedModelFilename)\n",
    "\n",
    "session.addDependency(persistedModel+\".jar\")  // External JAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The final step before scoring the test dataset is to instruct Snowpark to create a new UDF so the scoring function is available in Snowflake. In contrast to the UDFs in Part 2, this time we follow the initialization pattern mentioned above. This pattern is useful when a UDF has some initialization code (like in this example when the model file needs to be loaded) and ensures that the initialization code is executed only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "addClass(session,\"weka.classifiers.Classifier\")\n",
    "\n",
    "val scoreCls = new WekaRemoteHelper(persistedModel)\n",
    "\n",
    "val scoreUdf = session.udf.registerTemporary((r:String) => scoreCls.scoreRow(r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are now ready to score the test dataset. Scoring the training data is now as easy as:\n",
    "\n",
    "- creating a DataFrame for the test table\n",
    "- adding a column for the serialized representation of the data row\n",
    "- adding a column for the result returned by the scoring function\n",
    "- storing the DataFrame in a table in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val testDf = session.table(testTable)\n",
    "\n",
    "val resultDf = testDf\n",
    "    .withColumns(Seq(\"ARFFDATA\", \"PREDICTION\"), Seq(WekaClientHelper.concatAllCol(testDf,trainExcludeCols), scoreUdf(WekaClientHelper.concatAllCol(testDf,trainExcludeCols))))\n",
    "    .select(scoreKeyColumn, \"ARFFDATA\", predictColumn, \"PREDICTION\")\n",
    "\n",
    "resultDf.show()\n",
    "\n",
    "val res = resultDf.write.mode(SaveMode.Overwrite).saveAsTable(outTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The easiest way to evaluate the results from the scoring step is to create a confusion matrix. This query uses a pivot function. It result is a 2 x 2 matrix with values for positive scores (yes-yes/no-no) and false positives (yes-no/no-yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import com.snowflake.snowpark.types._\n",
    "\n",
    "val pivotValues = Seq(\"No\",\"Yes\")\n",
    "\n",
    "val resultDf=session.table(outTable)\n",
    "    .select(col(predictColumn),col(\"PREDICTION\"))\n",
    "    .pivot(col(predictColumn),pivotValues)\n",
    "    .agg(count(col(predictColumn)))\n",
    "    .sort(col(\"PREDICTION\"))\n",
    "\n",
    "resultDf\n",
    "    .schema\n",
    "\n",
    "val data=resultDf\n",
    "    .collect()\n",
    "    .map(r=>r.toSeq.drop(1).map(r=>r.toString().toDouble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "//\n",
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "\n",
    "plot(Seq(\n",
    "    Heatmap()\n",
    "      .withZ(\n",
    "        Seq(\n",
    "          data(0),\n",
    "          data(1),\n",
    "        )\n",
    "      )\n",
    "      .withColorscale(ColorScale.NamedScale(\"YIGnBu\"))\n",
    "      .withX(pivotValues)\n",
    "      .withY(pivotValues))\n",
    "    ,Layout(\n",
    "         title=\"Confusion Matrix\",\n",
    "         xaxis=Axis(title=predictColumn),\n",
    "         yaxis=Axis(title=\"Prediction\"))\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this three part series on Snowpark on Snowflake. In this series, we have used the:\n",
    "\n",
    "- Quick Start guide to set up a Jupyter Notebook for Snowpark on Snowflake \n",
    "- Snowpark DataFrames to simplify ETL/ELT/data engineering tasks\n",
    "- Simple and complex UDFs to execute custom code and third party libraries directly in Snowflake\n",
    "- Visualize results through a powerful graphics package \n",
    "\n",
    "We have seen how Snowpark brings the ability to execute complex ETL/ELT pipelines directly in Snowflake via the Snowflake DataFrame API. In contrast to processing data with Spark, which requires a Spark or EMR cluster, Snowpark does not require additional infrastructure. Lastly, with the ability to code in Scala we bring custom logic as well as third party libraries to the data rather than the data to the processing tier."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sL1Vq6r6J6dA",
    "zkvv6ruORQE9"
   ],
   "name": "ML_Training_and_Scoring_in_Scala_(Telco).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
