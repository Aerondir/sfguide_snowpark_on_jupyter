{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHItaUbRB74u"
   },
   "source": [
    "![](../../jpg/stock_small.jpg)\n",
    "\n",
    "\n",
    "This is the first notebook of a series to show how to use Snowpark on Snowflake. This notebook provides a quick-start guide and an introduction to the Snowpark DataFrame API. The notebook explains the steps for setting up the environment (REPL), and how to resolve dependencies to Snowpark. After a simple \"Hello World\" example you will learn about the Snowflake DataFrame API, projections, filters, and joins.\n",
    "\n",
    "In this notebook we will learn how to \n",
    "\n",
    "- [Quick Start](#Quick-Start): Set up the environment\n",
    "- [Hello World](#Hello-World): First steps \n",
    "- [Snowflake DataFrame API](#Snowflake-Dataframes): Query the Snowflake Sample Datasets via Snowflake DataFrames\n",
    "- [Conclusion](#Conclusion): Conclusion and What's next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To get started you need a Snowflake account and read/write access to a database. If you do not have a Snowflake account, you can sign up for a [free trial](https://signup.snowflake.com/). It doesn't even require a credit card.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we have to set up the environment for our notebook. The instructions for setting up the environment are in the Snowpark documentation in section [configuring-the-jupyter-notebook-for-snowpark](https://docs.snowflake.com/en/developer-guide/snowpark/quickstart-jupyter.html#configuring-the-jupyter-notebook-for-snowpark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 1\n",
    "\n",
    "Configure the notebook to use a Maven repository for a library that Snowpark depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:43:54.757143Z",
     "start_time": "2021-08-31T18:43:51.533Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val osgeoRepo = coursierapi.MavenRepository.of(\"https://repo.osgeo.org/repository/release\")\n",
    "interp.repositories() ++= Seq(osgeoRepo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 2\n",
    "\n",
    "Create a directory (if it doesn't exist) for temporary files created by the [REPL](https://ammonite.io/#Ammonite-REPL) environment. To avoid any side effects from previous runs, we also delete any files in that directory.\n",
    "\n",
    "**Note: Make sure that you have the operating system permissions to create a directory in that location.**\n",
    "\n",
    "**Note: If you are using multiple notebooks, youâ€™ll need to create and configure a separate REPL class directory for each notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:43:55.551209Z",
     "start_time": "2021-08-31T18:43:51.535Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ammonite.ops._\n",
    "import ammonite.ops.ImplicitWd._\n",
    "\n",
    "// This folder is used to store generated REPL classes, which will later be used in UDFs.\n",
    "// Please provide an empty folder path. This is essential for Snowpark UDFs to work\n",
    "val replClassPath = pwd+\"/repl_classes\"\n",
    "\n",
    "// Delete any old files in the directory.\n",
    "import sys.process._\n",
    "s\"rm -rf $replClassPath\" !\n",
    "\n",
    "// Create the REPL class folder.\n",
    "import sys.process._\n",
    "s\"mkdir -p $replClassPath\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 3\n",
    "\n",
    "Configure the compiler for the Scala REPL. This does the following:\n",
    "- Configures the compiler to generate classes for the REPL in the directory that you created earlier.\n",
    "- Configures the compiler to wrap code entered in the REPL in classes, rather than in objects.\n",
    "- Adds the directory that you created earlier as a dependency of the REPL interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:43:56.216552Z",
     "start_time": "2021-08-31T18:43:51.537Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "// Generate all repl classes in REPL class folder\n",
    "interp.configureCompiler(_.settings.outputDirs.setSingleOutput(replClassPath))\n",
    "interp.configureCompiler(_.settings.Yreplclassbased.value = true)\n",
    "interp.load.cp(os.Path(replClassPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 4\n",
    "\n",
    "Import the Snowpark library from Maven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:43:57.885255Z",
     "start_time": "2021-08-31T18:43:51.539Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`com.snowflake:snowpark:0.9.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To create a session, we need to authenticate ourselves to the Snowflake instance. Though it might be tempting to just override the authentication variables below with hard coded values, it's not considered best practice to do so. If you  share your version of the notebook, you might disclose your credentials by mistake to the recipient. Even worse, if you upload your notebook to a public code repository, you might advertise your credentials to the whole world. To prevent that, you should keep your credentials in an external file (like we are doing here).\n",
    "\n",
    "Then, update your credentials in that file and they will be saved on your local machine. Even better would be to switch from user/password authentication to [private key authentication](https://docs.snowflake.com/en/user-guide/key-pair-auth.html). \n",
    "\n",
    "Copy the credentials template file creds/template_credentials.txt to creds/credentials.txt and update the file with your credentials. Put your key files into the same directory or update the location in your credentials file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:44:05.103501Z",
     "start_time": "2021-08-31T18:43:51.540Z"
    },
    "hidden": true,
    "id": "kGGUvHHYjiGj"
   },
   "outputs": [],
   "source": [
    "import com.snowflake.snowpark._\n",
    "import com.snowflake.snowpark.functions._\n",
    "\n",
    "val session = Session.builder.configFile(\"creds/credentials.txt\").create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 5\n",
    "\n",
    "Add the Ammonite kernel classes as dependencies for your UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:44:05.556597Z",
     "start_time": "2021-08-31T18:43:51.542Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def addClass(session: Session, className: String): String = {\n",
    "  var cls1 = Class.forName(className)\n",
    "  val resourceName = \"/\" + cls1.getName().replace(\".\", \"/\") + \".class\"\n",
    "  val url = cls1.getResource(resourceName)\n",
    "  val path = url.getPath().split(\":\").last.split(\"!\").head\n",
    "  session.addDependency(path)\n",
    "  path\n",
    "}\n",
    "addClass(session, \"ammonite.repl.ReplBridge$\")\n",
    "addClass(session, \"ammonite.interp.api.APIHolder\")\n",
    "addClass(session, \"pprint.TPrintColors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Congratulations! You have successfully connected from a Jupyter Notebook to a Snowflake instance. Now we are ready to write our first \"Hello World\" program using Snowpark. That is as easy as the line in the cell below. After you have executed the cell below, you should see an output similar to\n",
    "\n",
    "    [scala-interpreter-1]  INFO (Logging.scala:22) - Actively querying parameter snowpark_lazy_analysis from server.\n",
    "    [scala-interpreter-1]  INFO (Logging.scala:22) - Execute query [queryID: 019e28e6-05025203-0000-22410336b00a]  SELECT  *  FROM (SELECT 'Hello World!' greeting) LIMIT 10\n",
    "    ----------------\n",
    "    |\"GREETING\"    |\n",
    "    ----------------\n",
    "    |Hello World!  |\n",
    "    ----------------\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:44:06.305653Z",
     "start_time": "2021-08-31T18:43:51.544Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "session.sql(\"SELECT 'Hello World!' greeting\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that Snowpark has automatically translated the Scala code into the familiar \"Hello World!\" SQL statement. This means that we can execute arbitrary SQL by using the **sql** method of the **session** class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, this doesn't really show the power of the new Snowpark API. At this point it's time to review the [Snowpark API documentation](https://docs.snowflake.com/en/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/index.html). It provides valuable information on how to use the Snowpark API. \n",
    "\n",
    "Let's now create a new *Hello World!* cell, that uses the Snowpark API, specifically the [DataFrame API](https://docs.snowflake.com/en/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/index.html?search=dataframe). To use the DataFrame API we first create a row and a schema and then a DataFrame based on the row and the schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:44:19.205383Z",
     "start_time": "2021-08-31T18:43:51.546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import com.snowflake.snowpark.types._\n",
    "\n",
    "val helloWorldDf=session.createDataFrame(Seq((\"Hello World!\"))).toDF(\"Greeting\")\n",
    "\n",
    "helloWorldDf\n",
    "   .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Snowflake DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T21:14:53.681667Z",
     "start_time": "2021-08-09T21:14:53.647Z"
    },
    "hidden": true
   },
   "source": [
    "After having mastered the *Hello World!* stage, we now can query Snowflake tables using the DataFrame API. To do so, we will query the [Snowflake Sample Database](https://docs.snowflake.com/en/user-guide/sample-data.html) included in any Snowflake instance. \n",
    "\n",
    "As you may know, the TPCH data sets come in different sizes from 1 TB to 1 PB (1000 TB). For starters we will query the orders table in the 10 TB dataset size. Instead of writing a SQL statement we will use the DataFrame API. The advantage is that DataFrames can be built as a pipeline. Let's take a look at the demoOrdersDf.\n",
    "\n",
    "    val demoOrdersDf=session.table(demoDataSchema :+ \"ORDERS\")\n",
    "    \n",
    "In contrast to the initial *Hello World!* example above, we now map a Snowflake table to a DataFrame. The definition of a DataFrame doesn't take any time. It's just defining metadata. To get the result, for instance the content of the Orders table, we need to [evaluate](https://docs.snowflake.com/en/developer-guide/snowpark/working-with-dataframes.html#performing-an-action-to-evaluate-a-dataframe) the DataFrame. One way of doing that is to apply the *[count()](https://docs.snowflake.com/en/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/DataFrame.html?search=count)* action which returns the row count of the dataframe. In this case, the row count of the *Orders* table. Another method is the *[schema](https://docs.snowflake.com/en/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/DataFrame.html?search=schema)* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:44:20.963152Z",
     "start_time": "2021-08-31T18:43:51.548Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val size:String=\"10\"\n",
    "val demoDataSchema:Seq[String]=Seq(\"SNOWFLAKE_SAMPLE_DATA\",\"TPCH_SF\"+size)\n",
    "val demoOrdersDf=session.table(demoDataSchema :+ \"ORDERS\")\n",
    "\n",
    "demoOrdersDf\n",
    "    .count()\n",
    "demoOrdersDf\n",
    "    .schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we want to apply a projection. In SQL terms, this is the *select* clause. Instead of getting all of the columns in the Orders table, we are only interested in a few. This is accomplished by the *[select()](https://docs.snowflake.com/en/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/DataFrame.html?search=select)* transformation. Note that we can just add additional qualifications to the already existing DataFrame of *demoOrdersDf* and create a new DataFrame that includes only a subset of columns. Lastly, instead of counting the rows in the DataFrame, this time we want to see the content of the DataFrame. To do so we need to evaluate the DataFrame. We can do that using another action *[show](https://docs.snowflake.com/en/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/DataFrame.html?search=show)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:44:21.475866Z",
     "start_time": "2021-08-31T18:43:51.550Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val demoOrdersProjectedDf=\n",
    "        demoOrdersDf\n",
    "            .select(col(\"O_ORDERKEY\"), col(\"O_CUSTKEY\"), col(\"O_ORDERSTATUS\"), col(\"O_ORDERDATE\"))\n",
    "\n",
    "demoOrdersProjectedDf\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's now assume that we do not want **all** the rows but only a subset of rows in a DataFrame. We can accomplish that with the *[filter()](https://docs.snowflake.com/en/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/DataFrame.html?search=select)* transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:44:22.005503Z",
     "start_time": "2021-08-31T18:43:51.552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val demoOrdersProjectedFilteredDf=\n",
    "        demoOrdersProjectedDf\n",
    "            .filter(col(\"O_ORDERSTATUS\")===lit(\"O\"))\n",
    "                    \n",
    "demoOrdersProjectedFilteredDf\n",
    "    .sort(col(\"O_ORDERKEY\"))\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And lastly, we want to create a new DataFrame which joins the Orders table with the LineItem table. Again, we are using our previous DataFrame that is a projection and a filter against the Orders table. We can join that DataFrame to the *LineItem* table and create a new DataFrame. We then apply the *[select()](https://docs.snowflake.com/en/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/DataFrame.html?search=filter)* transformation. Again, to see the result we need to evaluate the DataFrame, for instance by using the *show()* action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:44:25.752001Z",
     "start_time": "2021-08-31T18:43:51.554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val demoLinesDf=session.table(demoDataSchema :+ \"LINEITEM\")\n",
    "val demoOrdersLinesDf=\n",
    "        demoOrdersProjectedFilteredDf\n",
    "            .join(demoLinesDf,col(\"L_ORDERKEY\")===col(\"O_ORDERKEY\"))\n",
    "            .select(col(\"O_ORDERKEY\"),col(\"O_ORDERSTATUS\"),col(\"L_LINENUMBER\"),col(\"L_LINESTATUS\"))\n",
    "\n",
    "demoOrdersLinesDf\n",
    "    .sort(col(\"O_ORDERKEY\"),col(\"L_LINENUMBER\"))\n",
    "    .show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T18:44:26.852100Z",
     "start_time": "2021-08-31T18:43:51.555Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val demoOrdersCountByDateAndStatus=\n",
    "        demoOrdersProjectedDf\n",
    "            .select(col(\"O_ORDERDATE\"),col(\"O_ORDERSTATUS\"), col(\"O_ORDERKEY\"))\n",
    "            .groupBy(col(\"O_ORDERDATE\"),col(\"O_ORDERSTATUS\"))\n",
    "            .agg(count(col(\"O_ORDERKEY\")).name(\"O_COUNT\"))\n",
    "\n",
    "val demoOrdersCountByDateAndStatusArr=demoOrdersCountByDateAndStatus\n",
    "    .sort(col(\"O_ORDERDATE\"),col(\"O_ORDERSTATUS\"))\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowpark is a brand new developer experience that brings scalable data processing to the Data Cloud. In Part1 of this series, we learned how to set up a Jupyter Notebook and configure it to use Snowpark to connect to the Data Cloud. Next, we built a simple *Hello World!* program to test connectivity using embedded SQL. Then we enhanced that program by introducing the Snowpark Dataframe API. Lastly we explored the power of the Snowpark Dataframe API using filter, projection, and join transformations. \n",
    "\n",
    "In the next post of this series, we will learn how to create custom Scala based functions and execute arbitrary logic directly in Snowflake using user defined functions (UDFs) just by defining the logic in a Jupyter Notebook!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sL1Vq6r6J6dA",
    "zkvv6ruORQE9"
   ],
   "name": "ML_Training_and_Scoring_in_Scala_(Telco).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
