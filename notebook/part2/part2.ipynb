{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHItaUbRB74u"
   },
   "source": [
    "![](../../jpg/stock_small.jpg)\n",
    "\n",
    "This is the second notebook in the series. It builds on the quick-start of the first part. Using the TPCH dataset in the sample database, we will learn how to use aggregations and pivot functions in the Snowpark DataFrame API. Then, it introduces user definde functions (UDFs) and how to build a stand-alone UDF: a UDF that only uses standard primitives. From there, we will learn how to use third party Scala libraries to perform much more complex tasks like math for numbers with unbounded (unlimited number of significant digits) precision and how to perform sentiment analysis on an arbitrary string. \n",
    "\n",
    "In this session, the focus will be on:\n",
    "\n",
    "- [Advanced API features and visualization](#Advanced-API-Features-and-Data-Visualization)\n",
    "- [User-defined functions](#User-Defined-Functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To get started you need a Snowflake account and read/write access to a database. If you do not have a Snowflake account, you can sign up for a [free trial](https://signup.snowflake.com/). It doesn't even require a credit card.\n",
    "\n",
    "Next, you need access to a Jupyter Notebook environment running a Scala kernel and Ammonite REPL. If you do not already have access to that type of environment, I would highly recommend that you use [Snowtire V2](https://github.com/zoharsan/snowtire_v2) and this excellent post [From Zero to Snowpark in 5 minutes](https://medium.com/snowflake/from-zero-to-snowpark-in-5-minutes-72c5f8ec0b55). Please note that Snowtire is not officially supported by Snowflake, and is provided as-is. Additional instructions on how to set up the environment with the latest versions can be found in the README file in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we have to set up the environment for our notebook. The instructions for setting up the environment are in the Snowpark documentation in section [configuring-the-jupyter-notebook-for-snowpark](https://docs.snowflake.com/en/developer-guide/snowpark/quickstart-jupyter.html#configuring-the-jupyter-notebook-for-snowpark).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 1\n",
    "\n",
    "Configure the notebook to use a Maven repository for a library that Snowpark depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:57:43.070610Z",
     "start_time": "2021-08-30T21:57:39.978Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val osgeoRepo = coursierapi.MavenRepository.of(\"https://repo.osgeo.org/repository/release\")\n",
    "interp.repositories() ++= Seq(osgeoRepo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 2\n",
    "\n",
    "Create a directory (if it doesn't exist) for temporary files created by the [REPL](https://ammonite.io/#Ammonite-REPL) environment. To avoid any side effects from previous runs, we also delete any files in that directory.\n",
    "\n",
    "**Note: Make sure that you have the operating system permissions to create a directory in that location.**\n",
    "\n",
    "**Note: If you are using multiple notebooks, youâ€™ll need to create and configure a separate REPL class directory for each notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:57:43.904685Z",
     "start_time": "2021-08-30T21:57:40.020Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ammonite.ops._\n",
    "import ammonite.ops.ImplicitWd._\n",
    "\n",
    "// This folder is used to store generated REPL classes, which will later be used in UDFs.\n",
    "// Please provide an empty folder path. This is essential for Snowpark UDFs to work\n",
    "val replClassPath = pwd+\"/repl_classes\"\n",
    "\n",
    "// Delete any old files in the directory.\n",
    "import sys.process._\n",
    "s\"rm -rf $replClassPath\" !\n",
    "\n",
    "// Create the REPL class folder.\n",
    "import sys.process._\n",
    "s\"mkdir -p $replClassPath\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 3\n",
    "\n",
    "Configure the compiler for the Scala REPL. This does the following:\n",
    "- Configures the compiler to generate classes for the REPL in the directory that you created earlier.\n",
    "- Configures the compiler to wrap code entered in the REPL in classes, rather than in objects.\n",
    "- Adds the directory that you created earlier as a dependency of the REPL interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:57:44.405840Z",
     "start_time": "2021-08-30T21:57:40.063Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "// Generate all repl classes in REPL class folder\n",
    "interp.configureCompiler(_.settings.outputDirs.setSingleOutput(replClassPath))\n",
    "interp.configureCompiler(_.settings.Yreplclassbased.value = true)\n",
    "interp.load.cp(os.Path(replClassPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 4\n",
    "\n",
    "Import the Snowpark library from Maven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:57:45.848586Z",
     "start_time": "2021-08-30T21:57:40.106Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`com.snowflake:snowpark:0.8.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To create a session, we need to authenticate ourselves to the Snowflake instance. Though it might be tempting to just override the authentication variables below with hard coded values, it's not considered best practice to do so. If you  share your version of the notebook, you might disclose your credentials by mistake to the recipient. Even worse, if you upload your notebook to a public code repository, you might advertise your credentials to the whole world. To prevent that, you should keep your credentials in an external file (like we are doing here).\n",
    "\n",
    "Then, update your credentials in that file and they will be saved on your local machine. Even better would be to switch from user/password authentication to [private key authentication](https://docs.snowflake.com/en/user-guide/key-pair-auth.html). \n",
    "\n",
    "Copy the credentials template file creds/template_credentials.txt to creds/credentials.txt and update the file with your credentials. Put your key files into the same directory or update the location in your credentials file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:57:52.970262Z",
     "start_time": "2021-08-30T21:57:40.146Z"
    },
    "hidden": true,
    "id": "kGGUvHHYjiGj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import com.snowflake.snowpark._\n",
    "import com.snowflake.snowpark.functions._\n",
    "\n",
    "val session = Session.builder.configFile(\"creds/credentials.txt\").create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 5\n",
    "\n",
    "Add the Ammonite kernel classes as dependencies for your UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:57:53.299070Z",
     "start_time": "2021-08-30T21:57:40.189Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def addClass(session: Session, className: String): String = {\n",
    "  var cls1 = Class.forName(className)\n",
    "  val resourceName = \"/\" + cls1.getName().replace(\".\", \"/\") + \".class\"\n",
    "  val url = cls1.getResource(resourceName)\n",
    "  val path = url.getPath().split(\":\").last.split(\"!\").head\n",
    "  session.addDependency(path)\n",
    "  path\n",
    "}\n",
    "addClass(session, \"ammonite.repl.ReplBridge$\")\n",
    "addClass(session, \"ammonite.interp.api.APIHolder\")\n",
    "addClass(session, \"pprint.TPrintColors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 6\n",
    "\n",
    "For this exercise we need these additional libraries.\n",
    "- [plotl-scala (Plotly for Scala)](https://github.com/alexarchambault/plotly-scala)\n",
    "- [Spire (Numeric Abstractions for Scala)](https://typelevel.org/spire/)\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:57:54.657364Z",
     "start_time": "2021-08-30T21:57:40.233Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.8.2`\n",
    "import $ivy.`org.typelevel::spire:0.17.0`\n",
    "import $ivy.`edu.stanford.nlp:stanford-corenlp:4.2.2`\n",
    "\n",
    "import coursierapi._\n",
    "interp.load.ivy(\n",
    "      Dependency.of(\"edu.stanford.nlp\", \"stanford-corenlp\", \"4.2.2\").withClassifier(\"models\"),\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Advanced API Features and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We completed the first part of this series using the Snowpark DataFrame interface to project and filter datasets via the Snowpark DataFrame API. In the following section we will learn about more advanced features of the DataFrame API like aggregations and pivot. Secondly, instead of just producing tabular result sets, we will visualize the results using a graphics package called [plotly](https://github.com/alexarchambault/plotly-scala). \n",
    "\n",
    "The goal for this section is to produce a DataFrame that shows the count of open orders and filled orders by order date. The DataFrame should look similar to the matrix below.\n",
    "\n",
    "    -------------------------------------------------\n",
    "    |\"O_ORDERDATE\"  |\"OPEN_COUNT\"  |\"FILLED_COUNT\"  |\n",
    "    -------------------------------------------------\n",
    "    |1996-06-23     |6242          |0               |\n",
    "    |1995-12-05     |6306          |0               |\n",
    "    |1995-07-18     |6236          |0               |\n",
    "    |1994-09-06     |0             |6285            |\n",
    "    |1992-04-22     |0             |6341            |\n",
    "    -------------------------------------------------\n",
    "\n",
    "We then want to visualize those counts in a line chart. \n",
    "\n",
    "Let's see how we can do that in Snowpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:57:55.502785Z",
     "start_time": "2021-08-30T21:57:40.321Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val size:String=\"10\"\n",
    "val demoDataSchema:Seq[String]=Seq(\"SNOWFLAKE_SAMPLE_DATA\",\"TPCH_SF\"+size)\n",
    "val demoOrdersDf=session.table(demoDataSchema :+ \"ORDERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the next cell, we will filter the Orders by Status and only return Orders having the following status values. \n",
    "\n",
    "    open (\"O\") \n",
    "    filled (\"F\") \n",
    "    \n",
    "We then select only the 3 columns we are interested in.\n",
    "\n",
    "    O_ORDERDATE\n",
    "    O_ORDERSTATUS\n",
    "    O_ORDERKEY\n",
    "    \n",
    "Then we count the resulting rows by: \n",
    "\n",
    "    date \n",
    "    status\n",
    "    \n",
    "This returns the count of Orders by Date and Status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:57:56.651607Z",
     "start_time": "2021-08-30T21:57:40.368Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val demoOrdersGroupedDf=\n",
    "        demoOrdersDf\n",
    "            .filter((col(\"O_ORDERSTATUS\")===lit(\"O\") || col(\"O_ORDERSTATUS\") === \"F\"))\n",
    "            .select(col(\"O_ORDERDATE\"),col(\"O_ORDERSTATUS\"),col(\"O_ORDERKEY\"))\n",
    "            .groupBy(col(\"O_ORDERDATE\"),col(\"O_ORDERSTATUS\"))\n",
    "            .agg(count_distinct(col(\"O_ORDERKEY\")).name(\"O_COUNT\"))\n",
    "\n",
    "demoOrdersGroupedDf.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, to visualize the dataset it would be more convenient to have one row per day with the count of open and filled orders. We accomplish that by using the pivot function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:57:57.263401Z",
     "start_time": "2021-08-30T21:57:40.369Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val demoOrdersGroupedPivotDf=\n",
    "        demoOrdersGroupedDf\n",
    "            .pivot(col(\"O_ORDERSTATUS\"),Seq('F','O'))\n",
    "            .sum(col(\"O_COUNT\"))\n",
    "            .select(col(\"O_ORDERDATE\"),coalesce(col(\"'O'\"),lit(0)).name(\"OPEN_COUNT\"),coalesce(col(\"'F'\"),lit(0)).name(\"FILLED_COUNT\"))\n",
    "\n",
    "demoOrdersGroupedPivotDf.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Viola! You have created the data exactly as we designed it at the beginning. The last step is to visualize the data. For our visualization: \n",
    "\n",
    "In general, when we want to display a line chart for a single metric, we need 2 vectors of data. \n",
    "- The first vector is a sequence of strings, also called labels. \n",
    "- The second vector is a sequence of values, which have to be of data type double.\n",
    "If we have two metrics, we repeat the above structure of labels and values and wrap the two metrics in a sequence.\n",
    "\n",
    "So for our example we have the following structure:\n",
    "- Metric1: Open Orders\n",
    "  - Labels: Days\n",
    "  - Values: Open_Count\n",
    "- Metric2:\n",
    "  - Labels: Days\n",
    "  - Values: Filled_Count\n",
    "  \n",
    "As you can see, displaying the data is a breeze because we have prepared our data well. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:03.727551Z",
     "start_time": "2021-08-30T21:57:40.410Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "\n",
    "{\n",
    "    val data=demoOrdersGroupedPivotDf\n",
    "        .sort(col(\"O_ORDERDATE\"))\n",
    "        .collect()\n",
    "\n",
    "    val days=data.map(r => r(0).toString).toSeq\n",
    "    val open_count=data.map(r => r(1).toString.toDouble).toSeq\n",
    "    val filled_count=data.map(r => r(2).toString.toDouble).toSeq\n",
    "\n",
    "    plot(\n",
    "        Seq(\n",
    "          Scatter(days,open_count,name=\"OPEN_COUNT\")\n",
    "         ,Scatter(days,filled_count,name=\"FILLED_COUNT\"))\n",
    "         ,Layout(\n",
    "             title=\"Orders By Date\",\n",
    "             xaxis=Axis(title=\"Date\"),\n",
    "             yaxis=Axis(title=\"Orders per day\"),\n",
    "        )\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a more in depth discussion on the features of plotly, please review the [plotly documentation](https://github.com/alexarchambault/plotly-scala). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# User-Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section we will introduce another powerful feature in Snowpark, user-defined functions also called UDFs. Snowflake UDFs allow us to express arbitrary logic via Scala and execute that logic against massive datasets. The beauty is that the data doesn't have to move to the client machine but the necessary JAR files will be deployed to Snowflake automatically by Snowpark. We just have to let Snowpark know how to resolve the dependencies. And we will learn how to do that in this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Hello World\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's start with a very simple UDF that takes an input string and returns the same string as output. It's important to note that the object that encapsulates the reply method below \"extends\" trait Serializable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:03.892315Z",
     "start_time": "2021-08-30T21:57:40.522Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object simpleClass extends Serializable {\n",
    "    def reply(input:String):(String) = {\n",
    "        input\n",
    "    }   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can call the new object locally and it returns the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:04.098659Z",
     "start_time": "2021-08-30T21:57:40.523Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simpleClass.reply(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To create a UDF that can be called from within a DataFrame we have to map the method to a UDF via the session method *udf.registerTemporary*. A temporary method lives as long as the session exists and automatically gets removed when the session is closed. A temporary method is also not visible outside of the context of the current session. Creating a UDF tells Snowpark to upload all necessary JAR files into a stage and create a UDF to invoke the listed method from within the JAR file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:52:31.915702Z",
     "start_time": "2021-08-20T20:52:31.841Z"
    },
    "hidden": true
   },
   "source": [
    "**Note:** To create a permanent UDF you would use the *[udf.registerPermanent](https://docs.snowflake.com/en/developer-guide/snowpark/creating-udfs.html#creating-and-registering-a-named-udf)* session method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:15.828975Z",
     "start_time": "2021-08-30T21:57:40.597Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val greetingUdf=session.udf.registerTemporary((s:String) => simpleClass.reply(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With the UDF in place we can now create a DataFrame and call the UDF to compute the result value based on the parameters parsed into the UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:19.335219Z",
     "start_time": "2021-08-30T21:57:40.633Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val greetingDf=session.createDataFrame(Seq((\"Hello World\"))).toDF(\"greeting\")\n",
    "\n",
    "greetingDf\n",
    "    .withColumn(\"RESULT\",greetingUdf(col(\"greeting\")))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Processing of Rational Numbers with Arbitrary Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example we will learn how to process rational numbers with arbitrary precision. As you may know, Snowflake uses, besides other data types, double-precision (64 bit) IEEE 754 floating-point numbers. Check the Snowflake documentation regarding [numeric data types](https://docs.snowflake.com/en/sql-reference/data-types-numeric.html#float-float4-float8) for additional details.\n",
    "\n",
    "Let's look at the following example:\n",
    "\n",
    "        12345678909876543219999-12345678909876543210000\n",
    "        \n",
    "In the following statement we have 2 numbers in string representation. We will cast those numbers to a floating point representation and subtract them from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:20.431315Z",
     "start_time": "2021-08-30T21:57:40.670Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val longNumbers=session.createDataFrame(Seq((\"12345678909876543.9999\",\"12345678909876543.0000\"))).toDF(\"S1\",\"S2\")\n",
    "\n",
    "longNumbers\n",
    "    .withColumns(Seq(\"FP1\",\"FP2\"),Seq(callBuiltin(\"TO_DOUBLE\",col(\"S1\")),callBuiltin(\"TO_DOUBLE\",col(\"S2\"))))\n",
    "    .withColumn(\"RESULT\",col(\"FP1\")-col(\"FP2\"))\n",
    "    .select(col(\"FP1\"),col(\"FP2\"),col(\"RESULT\"))\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, instead of the expected 0.9999 we are getting 0. The reason is that (64 bit) IEEE 754 floating-point numbers can process a 23 digit number, though they don't have enough significant digits to store and process the *exact* value. They can only store a limited number of significant digits. When the maximum number of significant digits is exceeded, a (64 bit) IEEE 754 floating-point number loses precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To avoid that problem we have to use numbers with an unbounded precision. The OSS package [Spire](https://typelevel.org/spire/guide.html) addresses exactly that problem.\n",
    "\n",
    "In the following example I'll show how easy it is to take advantage of Spire to enjoy lossless processing of numbers with unbounded precision. Note that we have already imported the package in [Step 6 of the Quick Start](#Step-6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Creating a UDF in Snowpark is straightforward. Check [creating UDFs in Jupyter notebooks](https://docs.snowflake.com/en/developer-guide/snowpark/creating-udfs.html#creating-udfs-in-jupyter-notebooks) for details. We will use Spire data type *Rational* and map the add/subtract/divide/multiply primitives to the corresponding Spire functions. As a convenience function I have added a cast from Rational to Decimal just in case we wanted to see the value of a Rational number with a specific precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:21.189586Z",
     "start_time": "2021-08-30T21:57:40.776Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object snowmath extends Serializable {\n",
    "    \n",
    "    import spire.algebra._\n",
    "    import spire.math._\n",
    "    import spire.implicits._\n",
    "    import java.math.{MathContext, RoundingMode}  \n",
    "    \n",
    "    def opRational (op:String,r1:String,r2:String):String = {\n",
    "        op match {\n",
    "            case \"add\"      => (Rational(r1)+Rational(r2)).toString()\n",
    "            case \"subtract\" => (Rational(r1)-Rational(r2)).toString()\n",
    "            case \"divide\"   => (Rational(r1)/Rational(r2)).toString()\n",
    "            case \"multiply\" => (Rational(r1)*Rational(r2)).toString()\n",
    "            case _ => \"op not found\"\n",
    "        }\n",
    "    }\n",
    "    def fromRationalToBigDecimal(precision:String,r:String):String = {\n",
    "        Rational(r).toBigDecimal(precision.toInt,RoundingMode.HALF_EVEN).toString()\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rational Numbers in Spire are represented as *numerator/denominator*, or n/d . This means, we have to change the first number to a corresponding expression:\n",
    "\n",
    "    12345678909876543.9999 => 123456789098765439999/10000\n",
    "    \n",
    "And voila, we get the expected result because Spire can handle rational numbers with unbounded precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:21.513633Z",
     "start_time": "2021-08-30T21:57:40.813Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "snowmath.opRational(\"subtract\",\"123456789098765439999/10000\",\"12345678909876543\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, the cell above was running on our local machine. To execute the object in Snowflake we have to define the dependencies for our object, and we have to create UDF mappings for the functions we want to call in Snowflake. Snowpark will then upload all necessary JAR files to Snowflake and create the necessary mapping function for calling the Scala object directly in SQL. \n",
    "\n",
    "**Note: This step could take some time depending on your internet connection speed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:40.436174Z",
     "start_time": "2021-08-30T21:57:40.855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "addClass(session,\"spire.math.Rational\")\n",
    "addClass(session,\"algebra.ring.Field\")\n",
    "addClass(session,\"cats.kernel.Order\")\n",
    "\n",
    "val opRationalUdf=session.udf.registerTemporary((op:String,fp1:String,fp2:String) => snowmath.opRational(op,fp1,fp2))\n",
    "val fromRationalToBigDecimalUdf=session.udf.registerTemporary((p:String,r:String) => snowmath.fromRationalToBigDecimal(p,r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To execute the opRationalUdf in a SQL statement: \n",
    "\n",
    "- we create a Snowpark DataFrame with our input numbers \n",
    "- we create a new column, which is the result from the computation\n",
    "- we create a another column casting the result to a decimal with 40 digit precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:44.305570Z",
     "start_time": "2021-08-30T21:57:40.902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val longNumbersDf=session.createDataFrame(Seq((\"123456789098765439999/10000\",\"12345678909876543\"))).toDF(\"R1\",\"R2\")\n",
    "\n",
    "longNumbersDf\n",
    "    .withColumn(\"RESULT\",opRationalUdf(lit(\"subtract\"),col(\"R1\"),col(\"R2\")))\n",
    "    .withColumn(\"RESULT_DECIMAL\",fromRationalToBigDecimalUdf(lit(\"40\"),col(\"RESULT\")))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see above, snowmath can handle rational numbers with unbounded precision. The decimal result is still a string, however it could be cast to a floating point number in Snowflake. Just remember that(64 bit) IEEE 754 floating-point numbers have a maximum number of significant digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This example is similar to the previous UDF example  though it shows a few more features: \n",
    "\n",
    "- more complex UDF code\n",
    "- building a complex return object\n",
    "- lazy loading of data files\n",
    "\n",
    "The goal is to compute the sentiment of a text segment using the [Stanford CoreNLP library]((https://stanfordnlp.github.io/CoreNLP/) and return a JSON object that lists the sentence and the sentiment as follows:\n",
    "\n",
    "    [\n",
    "        {\n",
    "            \"sentence\": \"Happy Days\",\n",
    "            \"sentiment\": \"Positive\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "**Note: We have already imported the package in Step 6 of the Quick Start.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T19:38:53.279365Z",
     "start_time": "2021-08-13T19:38:53.232Z"
    },
    "hidden": true
   },
   "source": [
    "Let's review the code in the cell below. The Sentiment object has 2 methods, *getSentiment* and *compute*. Method *getSentiment* is a private method and is not visible outside of object *Sentiment*. In contrast, method *compute* is a public method and therefore can be exposed via a UDF. \n",
    "\n",
    "To perform a sentiment analysis, we need a *StandfordCoreNLP* object. Initializing that object can take a long time. Instead of defining the object every single time we perform a sentiment analysis it is much more efficient to define the *StandfordCoreNLP* object outside of method *compute* as a class variable. This makes the code much more efficient because it is initialized only once during initial creation of our *Sentiment* object. Review the Snowflake documentation to learn more about how to [write initialization code for a UDF](https://docs.snowflake.com/en/developer-guide/snowpark/creating-udfs.html#writing-initialization-code-for-a-udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:44.729645Z",
     "start_time": "2021-08-30T21:57:40.979Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object Sentiment extends Serializable {\n",
    "\n",
    "    import java.io.File\n",
    "    import java.nio.charset.Charset\n",
    "    import java.util.Properties\n",
    "\n",
    "    import edu.stanford.nlp.coref.CorefCoreAnnotations\n",
    "    import edu.stanford.nlp.ling.CoreAnnotations\n",
    "    import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations\n",
    "    import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
    "    import edu.stanford.nlp.sentiment.SentimentCoreAnnotations\n",
    "    import edu.stanford.nlp.sentiment.SentimentCoreAnnotations.SentimentAnnotatedTree\n",
    "    import edu.stanford.nlp.util.CoreMap\n",
    "\n",
    "    import scala.collection.JavaConverters._\n",
    "    \n",
    "    import com.snowflake.snowpark.types._\n",
    "\n",
    "    private def getSentiment(sentiment: Int): String = sentiment match {\n",
    "        case x if x == 0 || x == 1 => \"Negative\"\n",
    "        case 2 => \"Neutral\"\n",
    "        case x if x == 3 || x == 4 => \"Positive\"\n",
    "    }\n",
    "\n",
    "    private val props: Properties = new Properties()\n",
    "    props.put(\"annotators\", \"tokenize, ssplit, parse, sentiment\")\n",
    " \n",
    "    private lazy val pipeline: StanfordCoreNLP = new StanfordCoreNLP(props)\n",
    "    \n",
    "    def compute (text:String):String = {\n",
    "\n",
    "        // create blank annotator\n",
    "        val document: Annotation = new Annotation(text)\n",
    "        \n",
    "        // run all annotators\n",
    "        pipeline.annotate(document)\n",
    "\n",
    "        val sentences: List[CoreMap] = document.get(classOf[CoreAnnotations.SentencesAnnotation]).asScala.toList\n",
    "\n",
    "        \"[\\n\" + \n",
    "            sentences\n",
    "              .map(sentence => (sentence, sentence.get(classOf[SentimentAnnotatedTree])))\n",
    "              .map { case (sentence, tree) => \"{\\n\"+\n",
    "                                                \"  \\\"sentence\\\": \\\"\"+sentence.toString + \"\\\",\\n\" + \n",
    "                                                \"  \\\"sentiment\\\": \\\"\"+getSentiment(RNNCoreAnnotations.getPredictedClass(tree))+ \"\\\"\\n\" +\n",
    "                                               \"}\"\n",
    "                   }\n",
    "              .mkString(\",\") +\n",
    "        \"\\n]\"\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's test our Scala object locally with the text below.\n",
    "\n",
    "\"Mikail Farrar, a Georgia FedEx carrier who asked the universe for help and Tony Hawk responded. It's sweeter than it sounds. Farrar was on his usual route this week in the Atlanta suburbs when a 6-year-old boy chased him down and asked him to send a skateboard to Tony Hawk. He didn't have the half-pipe hero's address, so Farrar took a chance and tried to reach Hawk through TikTok. It worked! The social media-savvy skateboarder coordinated with Farrar to send the boy's skateboard to Hawk's correct address and send the 6-year-old a new board. It's just further proof that people can be wonderful and that Tony Hawk is as kind as he is gnarly, dude.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:58:51.450743Z",
     "start_time": "2021-08-30T21:57:40.981Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val text=\"\"\"\n",
    "Mikail Farrar, a Georgia FedEx carrier who asked the universe for help and Tony Hawk responded. It's sweeter than it sounds. Farrar was on his usual route this week in the Atlanta suburbs when a 6-year-old boy chased him down and asked him to send a skateboard to Tony Hawk. He didn't have the half-pipe hero's address, so Farrar took a chance and tried to reach Hawk through TikTok. It worked! The social media-savvy skateboarder coordinated with Farrar to send the boy's skateboard to Hawk's correct address and send the 6-year-old a new board. It's just further proof that people can be wonderful and that Tony Hawk is as kind as he is gnarly, dude.\n",
    "\"\"\"\n",
    "Sentiment.compute(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we have seen in the Spire example, all JAR files required for our UDF (just pick any class in a specific JAR as a proxy for the whole JAR) have to be declared as dependencies. *addClass* performs that step. In this particular case, we also have a dependency on a file storing the sentiment models. That file, the *Models* file, will be listed separately using the *addDependency* command because it doesn't implement any classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T22:00:35.723741Z",
     "start_time": "2021-08-30T21:57:40.982Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "addClass(session,\"edu.stanford.nlp.pipeline.StanfordCoreNLP\")\n",
    "addClass(session,\"org.ejml.simple.SimpleBase\")\n",
    "addClass(session,\"org.ejml.data.Matrix\")\n",
    "addClass(session,\"org.ejml.dense.row.CommonOps_DDRM\")\n",
    "\n",
    "session.addDependency(\"/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/edu/stanford/nlp/stanford-corenlp/4.2.2/stanford-corenlp-4.2.2-models.jar\")\n",
    "\n",
    "val sentimentUdf=udf((s:String) => Sentiment.compute(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T22:00:44.938061Z",
     "start_time": "2021-08-30T21:57:40.984Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val sourceTextDf=session.createDataFrame(Seq(text)).toDF(\"TEXT\")\n",
    "\n",
    "sourceTextDf\n",
    "    .withColumn(\"sentiment\",callBuiltin(\"parse_json\",sentimentUdf(col(\"TEXT\"))))\n",
    "    .select(col(\"sentiment\"))\n",
    "    .show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the series we learned how to use more complex functions in the Snowflake DataFrame API and how visualize datasets directly in a Jupyter Notebook using the plotly-scala library. We also learned how easy it is to enhance Snowflakes capabilities by creating Scala UDFs with third party libraries and arbitrary custom code. \n",
    "\n",
    "In the last installment we will combine all of the features we have seen so far and will build a solution for an end-end machine learning use case using the Weka ML library."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sL1Vq6r6J6dA",
    "zkvv6ruORQE9"
   ],
   "name": "ML_Training_and_Scoring_in_Scala_(Telco).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
