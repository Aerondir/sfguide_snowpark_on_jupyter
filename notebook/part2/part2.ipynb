{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHItaUbRB74u"
   },
   "source": [
    "![](../../jpg/stock_small.jpg)\n",
    "\n",
    "This is the second notebook in the series. It builds on the quick-start of the first part. Using the TPCH datset in the sample database, we will learn how to use aggregations and pivot functions in the Snowpark dataframe API. Then it introduces UDFs and how to build a stand-alone UDF, i.e. a UDF that only uses standard primitives. From there we will learn how to use thrid party Scala libraries to perform much more complex tasks like math for numbers with unbound (unlimited number of significant digits) precision or how to perform sentiment analysis on an arbitrary string. \n",
    "\n",
    "In this session, the focus will be\n",
    "\n",
    "- [Advanced API features and Visualization](#Advanced-API-features-and-Visualization)\n",
    "- [User Defined Functions](#User-Defined-Functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started you need a Snowflake account and read/write access to a database. If you do not have a Snowflake account, just follow the steps in [sign up](https://signup.snowflake.com/). A time limited  account for trial purposes is free and doesn't even require a credit card. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we have to set up the Environment for our Notebook. The instructions for setting up the environment are [here](https://docs.snowflake.com/en/developer-guide/snowpark/quickstart-jupyter.html#configuring-the-jupyter-notebook-for-snowpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 1\n",
    "\n",
    "Configure the notebook to use a Maven repository for a library that Snowpark depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:08.746526Z",
     "start_time": "2021-08-20T20:30:06.867Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val osgeoRepo = coursierapi.MavenRepository.of(\"https://repo.osgeo.org/repository/release\")\n",
    "interp.repositories() ++= Seq(osgeoRepo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 2\n",
    "\n",
    "Create a directory (if it doesn't exist) for temporary files created by the [REPL](https://ammonite.io/#Ammonite-REPL) environment. To avoid any side-effects from previous runs, we also delete any files that might exist in that directory.\n",
    "\n",
    "**Note: Make sure that you have the operating system permissions to create a directory in that location.**\n",
    "\n",
    "**Note: If you are using multiple notebooks, youâ€™ll need to create and configure a separate REPL class directory for each notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:09.347154Z",
     "start_time": "2021-08-20T20:30:06.869Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ammonite.ops._\n",
    "import ammonite.ops.ImplicitWd._\n",
    "\n",
    "// This folder is used to store generated repl classes, which will later be used in UDF.\n",
    "// Please provide an empty folder path.This is essential for Snowpark UDF to work\n",
    "val replClassPath = pwd+\"/repl_classes\"\n",
    "\n",
    "// delete any old files in the directory\n",
    "import sys.process._\n",
    "s\"rm -rf $replClassPath\" !\n",
    "\n",
    "// Create the repl class folder\n",
    "import sys.process._\n",
    "s\"mkdir -p $replClassPath\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 3\n",
    "\n",
    "Configure the compiler for the Scala REPL. This does the following:\n",
    "- Configures the compiler to generate classes for the REPL in the directory that you created earlier.\n",
    "- Configures the compiler to wrap code entered in the REPL in classes, rather than in objects.\n",
    "- Adds the directory that you created earlier as a dependency of the REPL interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:09.849399Z",
     "start_time": "2021-08-20T20:30:06.870Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "// Generate all repl classes in repl class folder\n",
    "interp.configureCompiler(_.settings.outputDirs.setSingleOutput(replClassPath))\n",
    "interp.configureCompiler(_.settings.Yreplclassbased.value = true)\n",
    "interp.load.cp(os.Path(replClassPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 4\n",
    "Import the Snowpark library from Maven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:10.905652Z",
     "start_time": "2021-08-20T20:30:06.872Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`com.snowflake:snowpark:0.8.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To create a session we need to authenticate ourselves to the Snowflake instance. Though it might be tempting to just override the authentication variables below with hard coded values, its not considered best practice to do so. In case you ever wanted to share your version of the notebook, your could disclose your credentials by mistake to the recipient. Even worse, if you upload your notebook to a public code repository, you might advertise your credentials to the whole wide world. To prevent that, you should keep your credentials in an external file (like we are doing here).\n",
    "\n",
    "Then update your credentials in that file and they will be save on your local machine. Even better if you do not use user/password authentication but [private key authentication](https://docs.snowflake.com/en/user-guide/key-pair-auth.html). \n",
    "\n",
    "Copy the credentials template file creds/template_credentials.txt to creds/credentials.txt and update the file with your credentials. Put your key files into the same directory or update the location accordingly in your credentials file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:15.703134Z",
     "start_time": "2021-08-20T20:30:06.873Z"
    },
    "hidden": true,
    "id": "kGGUvHHYjiGj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import com.snowflake.snowpark._\n",
    "import com.snowflake.snowpark.functions._\n",
    "\n",
    "val session = Session.builder.configFile(\"creds/credentials.txt\").create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 5\n",
    "Add the Ammonite kernel classes as dependencies for your UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:16.068394Z",
     "start_time": "2021-08-20T20:30:06.875Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def addClass(session: Session, className: String): String = {\n",
    "  var cls1 = Class.forName(className)\n",
    "  val resourceName = \"/\" + cls1.getName().replace(\".\", \"/\") + \".class\"\n",
    "  val url = cls1.getResource(resourceName)\n",
    "  val path = url.getPath().split(\":\").last.split(\"!\").head\n",
    "  session.addDependency(path)\n",
    "  path\n",
    "}\n",
    "addClass(session, \"ammonite.repl.ReplBridge$\")\n",
    "addClass(session, \"ammonite.interp.api.APIHolder\")\n",
    "addClass(session, \"pprint.TPrintColors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 6\n",
    "\n",
    "For this exercise we need three additional libraries\n",
    "- [plotl-scala (Plotly for Scala)](https://github.com/alexarchambault/plotly-scala)\n",
    "- [Spire (Numeric Abstractions for Scala)](https://typelevel.org/spire/)\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:17.401480Z",
     "start_time": "2021-08-20T20:30:06.876Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.8.2`\n",
    "import $ivy.`org.typelevel::spire:0.17.0`\n",
    "import $ivy.`edu.stanford.nlp:stanford-corenlp:4.2.2`\n",
    "\n",
    "import coursierapi._\n",
    "interp.load.ivy(\n",
    "      Dependency.of(\"edu.stanford.nlp\", \"stanford-corenlp\", \"4.2.2\").withClassifier(\"models\"),\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Advanced API features and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We completed the first part of this series using the Snowpark dataframe interface to project and filter datasets via the Snowpark databframe API. In the following section we will learn about more advanced features of the dataframe API like aggregations and pivot. Secondly, instead of just producing tabular result sets, we will visualize the results using a graphics package called [plotly](https://github.com/alexarchambault/plotly-scala). \n",
    "\n",
    "The for this section is to produce a dataframe that shows the count of open orders, filled orders by order date. The dataaframe should look similar to the matrix below\n",
    "\n",
    "    -------------------------------------------------\n",
    "    |\"O_ORDERDATE\"  |\"OPEN_COUNT\"  |\"FILLED_COUNT\"  |\n",
    "    -------------------------------------------------\n",
    "    |1996-06-23     |6242          |0               |\n",
    "    |1995-12-05     |6306          |0               |\n",
    "    |1995-07-18     |6236          |0               |\n",
    "    |1994-09-06     |0             |6285            |\n",
    "    |1992-04-22     |0             |6341            |\n",
    "    -------------------------------------------------\n",
    "\n",
    "We then want to visualize those counts via a line chart. \n",
    "\n",
    "Let's see how we can do that in Snowpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:18.330435Z",
     "start_time": "2021-08-20T20:30:06.878Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val size:String=\"10\"\n",
    "val demoDataSchema:Seq[String]=Seq(\"SNOWFLAKE_SAMPLE_DATA\",\"TPCH_SF\"+size)\n",
    "val demoOrdersDf=session.table(demoDataSchema :+ \"ORDERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the next cell, we will filter the Orders by Status and only return Orders having the folling status values. \n",
    "\n",
    "    open (\"O\") \n",
    "    filled (\"F\") \n",
    "    \n",
    "We then select only the 3 columns we are interested in\n",
    "\n",
    "    O_ORDERDATE\n",
    "    O_ORDERSTATUS\n",
    "    O_ORDERKEY\n",
    "    \n",
    "Then we count the resulting rows by \n",
    "\n",
    "    date \n",
    "    status\n",
    "    \n",
    "This returns the count of Orders by Date and Status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:19.399714Z",
     "start_time": "2021-08-20T20:30:06.879Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val demoOrdersGroupedDf=\n",
    "        demoOrdersDf\n",
    "            .filter((col(\"O_ORDERSTATUS\")===lit(\"O\") || col(\"O_ORDERSTATUS\") === \"F\"))\n",
    "            .select(col(\"O_ORDERDATE\"),col(\"O_ORDERSTATUS\"),col(\"O_ORDERKEY\"))\n",
    "            .groupBy(col(\"O_ORDERDATE\"),col(\"O_ORDERSTATUS\"))\n",
    "            .agg(count_distinct(col(\"O_ORDERKEY\")).name(\"O_COUNT\"))\n",
    "\n",
    "demoOrdersGroupedDf.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, to visualize the dataset it would be more convenient to have one row per day with the count of open and filled orders. We accomplish that by using the pivot function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:20.136787Z",
     "start_time": "2021-08-20T20:30:06.881Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val demoOrdersGroupedPivotDf=\n",
    "        demoOrdersGroupedDf\n",
    "            .pivot(col(\"O_ORDERSTATUS\"),Seq('F','O'))\n",
    "            .sum(col(\"O_COUNT\"))\n",
    "            .select(col(\"O_ORDERDATE\"),coalesce(col(\"'O'\"),lit(0)).name(\"OPEN_COUNT\"),coalesce(col(\"'F'\"),lit(0)).name(\"FILLED_COUNT\"))\n",
    "\n",
    "demoOrdersGroupedPivotDf.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Viola! You hae created the data exactly as we designed it at the beginning. The last step is to visualize the data. \n",
    "\n",
    "In general, when we want to display a line chart for a single metric, we need 2 vectors of data. \n",
    "- The first vector is a Sequence of strings, also called Labels. \n",
    "- The second vector is a sequence of values, which have to be of data type double.\n",
    "If we have two metrics, we repeat the above structure of labels and values and wrap the two metrics in a sequence.\n",
    "\n",
    "So for our example we have the following structure:\n",
    "- Metric1: Open Orders\n",
    "  - Labels: Days\n",
    "  - Values: Open_Count\n",
    "- Metric2:\n",
    "  - Labels: Days\n",
    "  - Values: Filled_Count\n",
    "  \n",
    "As you can see, displaying the data is a breeze since we have prepared our data well. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:23.122221Z",
     "start_time": "2021-08-20T20:30:06.883Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "\n",
    "{\n",
    "    val data=demoOrdersGroupedPivotDf\n",
    "        .sort(col(\"O_ORDERDATE\"))\n",
    "        .collect()\n",
    "\n",
    "    val days=data.map(r => r(0).toString).toSeq\n",
    "    val open_count=data.map(r => r(1).toString.toDouble).toSeq\n",
    "    val filled_count=data.map(r => r(2).toString.toDouble).toSeq\n",
    "\n",
    "    plot(\n",
    "        Seq(\n",
    "          Scatter(days,open_count,name=\"OPEN_COUNT\")\n",
    "         ,Scatter(days,filled_count,name=\"FILLED_COUNT\"))\n",
    "         ,Layout(\n",
    "             title=\"Orders By Date\",\n",
    "             xaxis=Axis(title=\"Date\"),\n",
    "             yaxis=Axis(title=\"Orders per day\"),\n",
    "        )\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a more in depth discussions on the features of plotly, please review the [documentation](https://github.com/alexarchambault/plotly-scala). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# User Defined Funtions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the next section we will introduce another powerfull feature in Snowpark, i.e user defined functions also called UDFs. UDFs allow us to express arbitrary logic via Scala and execute that logic against massive datasets. The beauty is that the data doesn't have to move to the client machine but the necessary jar files will be deployed to Snowflake automatically by Snowpark. We just have to let Snowpark know how to resolve the dependencies. And we will learn how to do that in this section. \n",
    "\n",
    "Lets have a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Hello World\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's start with a very simple UDF that takes an input string and returns the same string as output. It's important to note that the object that encapsulates the reply method below \"extends\" trait Serializable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:32.921151Z",
     "start_time": "2021-08-20T20:30:32.757Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object simpleClass extends Serializable {\n",
    "    def reply(input:String):(String) = {\n",
    "        input\n",
    "    }   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can call the new object locally and it returns the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:30:36.904580Z",
     "start_time": "2021-08-20T20:30:36.695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simpleClass.reply(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To create a UDF that can be called from within a dataframe we have to map the method to a UDF via the session method *udf.registerTemporary*. A temporary method lives as long as the session exists and automatically get removed when the session is closed. A temporary method is also not visible outside of the context of the the current session. Creating a UDF instructs Snowpark to upload all necessary jar files into a stage and Snowflake and to create a UDF to invoke the listed mehod from within the jar file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:52:31.915702Z",
     "start_time": "2021-08-20T20:52:31.841Z"
    },
    "hidden": true
   },
   "source": [
    "**Note:** To create a permanent UDF we have to use the *[udf.registerPermanent](https://docs.snowflake.com/en/developer-guide/snowpark/creating-udfs.html#creating-and-registering-a-named-udf)* session method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:55:52.897346Z",
     "start_time": "2021-08-20T20:55:47.538Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val greetingUdf=session.udf.registerTemporary((s:String) => simpleClass.reply(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With the UDF in place we can now create a dataframe and call the UDF to compute the result value based on the parameters parsed into the UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T20:32:23.646143Z",
     "start_time": "2021-08-20T20:32:21.184Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val greetingDf=session.createDataFrame(Seq((\"Hello World\"))).toDF(\"greeting\")\n",
    "\n",
    "greetingDf\n",
    "    .withColumn(\"RESULT\",greetingUdf(col(\"greeting\")))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Processing of Rational Numbers with Arbitrary Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example we will learn how to process Rational numbers with arbitrary precission. As you may know, Snowflake uses, besides other data types, double-precision (64 bit) IEEE 754 floating-point numbers. More about this topic can be found [here](https://docs.snowflake.com/en/sql-reference/data-types-numeric.html#float-float4-float8).\n",
    "\n",
    "So lets look at the following example:\n",
    "\n",
    "        12345678909876543219999-12345678909876543210000\n",
    "        \n",
    "In the following statement we have 2 numbers in string representation. We will cast those numbers to a floating point representation and subtract them from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:07:57.478093Z",
     "start_time": "2021-08-18T20:07:55.888Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val longNumbers=session.createDataFrame(Seq((\"12345678909876543.9999\",\"12345678909876543.0000\"))).toDF(\"S1\",\"S2\")\n",
    "\n",
    "longNumbers\n",
    "    .withColumns(Seq(\"FP1\",\"FP2\"),Seq(callBuiltin(\"TO_DOUBLE\",col(\"S1\")),callBuiltin(\"TO_DOUBLE\",col(\"S2\"))))\n",
    "    .withColumn(\"RESULT\",col(\"FP1\")-col(\"FP2\"))\n",
    "    .select(col(\"FP1\"),col(\"FP2\"),col(\"RESULT\"))\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, instead of the expected 0.9999 we are getting 0. The reason is that (64 bit) IEEE 754 floating-point numbers can handle 16 digit long numbers, though they dont have enough significant digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So to avoid that problem we have to use numbers with an unbound precision. The OSS package [Spire](https://typelevel.org/spire/guide.html) addresses exactly that problem.\n",
    "\n",
    "In the following example I'll show how easy it is to take advantage of Spire to enjoy lossless processing of numbers with unbound precision. Note that we have already imported the package in [Step 6 of the Quick Start](#Step-6) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Creating UDF for Snowpark is straight forward. You can review the documentation [here](https://docs.snowflake.com/en/developer-guide/snowpark/creating-udfs.html#creating-udfs-in-jupyter-notebooks). Please note that the object / class needs to extend *Serializable*. For this example I am creating and object instead of a class since we need only one instance of the snowmath class (check [here](https://docs.scala-lang.org/tour/singleton-objects.html) for more details).\n",
    "\n",
    "The implemention of our object is very straight forward. We will use spire data type *Rational* and map the add/subtract/divide/multiply primitives to the corresponding spire functions. As a convenience function I have added a cast from Rational to Decimal just in case we wanted to see the value of a Rational number with a specific precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:08:12.580465Z",
     "start_time": "2021-08-18T20:08:11.950Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object snowmath extends Serializable {\n",
    "    \n",
    "    import spire.algebra._\n",
    "    import spire.math._\n",
    "    import spire.implicits._\n",
    "    import java.math.{MathContext, RoundingMode}  \n",
    "    \n",
    "    def opRational (op:String,r1:String,r2:String):String = {\n",
    "        op match {\n",
    "            case \"add\"      => (Rational(r1)+Rational(r2)).toString()\n",
    "            case \"subtract\" => (Rational(r1)-Rational(r2)).toString()\n",
    "            case \"divide\"   => (Rational(r1)/Rational(r2)).toString()\n",
    "            case \"multiply\" => (Rational(r1)*Rational(r2)).toString()\n",
    "            case _ => \"op not found\"\n",
    "        }\n",
    "    }\n",
    "    def fromRationalToBigDecimal(precision:String,r:String):String = {\n",
    "        Rational(r).toBigDecimal(precision.toInt,RoundingMode.HALF_EVEN).toString()\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rational Number in Spire are represented as n/d. This means, we have to change the first number to a corresponding expression\n",
    "\n",
    "    12345678909876543.9999 => 123456789098765439999/10000\n",
    "    \n",
    "And Voila, we get the expected result since spire can handle Rational numbers with unbound precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:08:14.006242Z",
     "start_time": "2021-08-18T20:08:13.604Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "snowmath.opRational(\"subtract\",\"123456789098765439999/10000\",\"12345678909876543\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, the cell above was running on our local machine. To execute the object in Snowflake we have to define the dependencies for our object above, and we have to create UDF mappings for the functions we want to call in Snowflake. Snowpark will then upload all necessary jar files to Snowflake and create the necessary mapping function for calling the scala object directly in SQL. This step could take some time depending on your ISP's upload speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:08:39.182099Z",
     "start_time": "2021-08-18T20:08:15.532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "addClass(session,\"spire.math.Rational\")\n",
    "addClass(session,\"algebra.ring.Field\")\n",
    "addClass(session,\"cats.kernel.Order\")\n",
    "\n",
    "val opRationalUdf=session.udf.registerTemporary((op:String,fp1:String,fp2:String) => snowmath.opRational(op,fp1,fp2))\n",
    "val fromRationalToBigDecimalUdf=session.udf.registerTemporary((p:String,r:String) => snowmath.fromRationalToBigDecimal(p,r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To execute the opRationalUdf in a sql statement, \n",
    "\n",
    "- we create a Snowpark dataframe with our input numbers, \n",
    "- we create a new column which is the result from the computation\n",
    "- we create a another column casting the result to a decimal with 40 digit precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:08:46.029736Z",
     "start_time": "2021-08-18T20:08:40.850Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val longNumbersDf=session.createDataFrame(Seq((\"123456789098765439999/10000\",\"12345678909876543\"))).toDF(\"R1\",\"R2\")\n",
    "\n",
    "longNumbersDf\n",
    "    .withColumn(\"RESULT\",opRationalUdf(lit(\"subtract\"),col(\"R1\"),col(\"R2\")))\n",
    "    .withColumn(\"RESULT_DECIMAL\",fromRationalToBigDecimalUdf(lit(\"40\"),col(\"RESULT\")))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see above, snowmath can handle Rational numbers with unbound precision. The decimal result is still a string, however it could be cast to a floating point number in Snowflake. Just remember, that Snowflake floating point numbers have a maximum number of significant digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This example is similar to the previous though it shows a couple more features. \n",
    "\n",
    "- more complex UDF code\n",
    "- building a complex return object\n",
    "- lazy loading of data files\n",
    "\n",
    "The goal is to compute the sentiment of a text segment using the [Stanford CoreNLP library]((https://stanfordnlp.github.io/CoreNLP/) and return a JSON object that lists the sentence and the sentiment as follows:\n",
    "\n",
    "    [\n",
    "        {\n",
    "            \"sentence\": \"Happy Days\",\n",
    "            \"sentiment\": \"Positive\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "Note that we have already imported the package in Step 6 of the Quick Start ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T19:38:53.279365Z",
     "start_time": "2021-08-13T19:38:53.232Z"
    },
    "hidden": true
   },
   "source": [
    "Note the definition of *pipeline* and the fact, that *pipeline* is a class variable, i.e. its defined outside of the UDF *compute*. The initilization code for *pipeline* will be executed upon the first invocation of the Sentiment object. This is important to understand, since Snowflake allows a certain amount of time for class initialization (currently 300 secs) but only 5 secs for the method execution. So if *pipeline* was initialized within the scope of the *compute* method, it most likely would exceed the 5 second timeout and fail when executed in Snowflake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:08:48.578354Z",
     "start_time": "2021-08-18T20:08:48.081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object Sentiment extends Serializable {\n",
    "\n",
    "    import java.io.File\n",
    "    import java.nio.charset.Charset\n",
    "    import java.util.Properties\n",
    "\n",
    "    import edu.stanford.nlp.coref.CorefCoreAnnotations\n",
    "    import edu.stanford.nlp.ling.CoreAnnotations\n",
    "    import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations\n",
    "    import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
    "    import edu.stanford.nlp.sentiment.SentimentCoreAnnotations\n",
    "    import edu.stanford.nlp.sentiment.SentimentCoreAnnotations.SentimentAnnotatedTree\n",
    "    import edu.stanford.nlp.util.CoreMap\n",
    "\n",
    "    import scala.collection.JavaConverters._\n",
    "    \n",
    "    import com.snowflake.snowpark.types._\n",
    "\n",
    "    private def getSentiment(sentiment: Int): String = sentiment match {\n",
    "        case x if x == 0 || x == 1 => \"Negative\"\n",
    "        case 2 => \"Neutral\"\n",
    "        case x if x == 3 || x == 4 => \"Positive\"\n",
    "    }\n",
    "\n",
    "    private val props: Properties = new Properties()\n",
    "    props.put(\"annotators\", \"tokenize, ssplit, parse, sentiment\")\n",
    " \n",
    "    private lazy val pipeline: StanfordCoreNLP = new StanfordCoreNLP(props)\n",
    "    \n",
    "    def compute (text:String):String = {\n",
    "\n",
    "        // create blank annotator\n",
    "        val document: Annotation = new Annotation(text)\n",
    "        \n",
    "        // run all Annotators\n",
    "        pipeline.annotate(document)\n",
    "\n",
    "        val sentences: List[CoreMap] = document.get(classOf[CoreAnnotations.SentencesAnnotation]).asScala.toList\n",
    "\n",
    "        \"[\\n\" + \n",
    "            sentences\n",
    "              .map(sentence => (sentence, sentence.get(classOf[SentimentAnnotatedTree])))\n",
    "              .map { case (sentence, tree) => \"{\\n\"+\n",
    "                                                \"  \\\"sentence\\\": \\\"\"+sentence.toString + \"\\\",\\n\" + \n",
    "                                                \"  \\\"sentiment\\\": \\\"\"+getSentiment(RNNCoreAnnotations.getPredictedClass(tree))+ \"\\\"\\n\" +\n",
    "                                               \"}\"\n",
    "                   }\n",
    "              .mkString(\",\") +\n",
    "        \"\\n]\"\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's test our Scala object locally with the text below.\n",
    "\n",
    "\"Mikail Farrar, a Georgia FedEx carrier who asked the universe for help and Tony Hawk responded. It's sweeter than it sounds. Farrar was on his usual route this week in the Atlanta suburbs when a 6-year-old boy chased him down and asked him to send a skateboard to Tony Hawk. He didn't have the half-pipe hero's address, so Farrar took a chance and tried to reach Hawk through TikTok. It worked! The social media-savvy skateboarder coordinated with Farrar to send the boy's skateboard to Hawk's correct address and send the 6-year-old a new board. It's just further proof that people can be wonderful and that Tony Hawk is as kind as he is gnarly, dude.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:08:55.124571Z",
     "start_time": "2021-08-18T20:08:48.902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val text=\"\"\"\n",
    "Mikail Farrar, a Georgia FedEx carrier who asked the universe for help and Tony Hawk responded. It's sweeter than it sounds. Farrar was on his usual route this week in the Atlanta suburbs when a 6-year-old boy chased him down and asked him to send a skateboard to Tony Hawk. He didn't have the half-pipe hero's address, so Farrar took a chance and tried to reach Hawk through TikTok. It worked! The social media-savvy skateboarder coordinated with Farrar to send the boy's skateboard to Hawk's correct address and send the 6-year-old a new board. It's just further proof that people can be wonderful and that Tony Hawk is as kind as he is gnarly, dude.\n",
    "\"\"\"\n",
    "Sentiment.compute(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we have seen in the Spire example, all jar files (one of the classes implemented in the jar is just a proxy for the whole jar) have to be declared as dependencies. In this particular case, we also have a file storing the models. That file, i.e. the *Models* file will be listed separately, since it doesn't implement any classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:10:42.885857Z",
     "start_time": "2021-08-18T20:08:52.392Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "addClass(session,\"edu.stanford.nlp.pipeline.StanfordCoreNLP\")\n",
    "addClass(session,\"org.ejml.simple.SimpleBase\")\n",
    "addClass(session,\"org.ejml.data.Matrix\")\n",
    "addClass(session,\"org.ejml.dense.row.CommonOps_DDRM\")\n",
    "\n",
    "session.addDependency(\"/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/edu/stanford/nlp/stanford-corenlp/4.2.2/stanford-corenlp-4.2.2-models.jar\")\n",
    "\n",
    "val sentimentUdf=udf((s:String) => Sentiment.compute(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:10:53.277459Z",
     "start_time": "2021-08-18T20:08:55.847Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val sourceTextDf=session.createDataFrame(Seq(text)).toDF(\"TEXT\")\n",
    "\n",
    "sourceTextDf\n",
    "    .withColumn(\"sentiment\",callBuiltin(\"parse_json\",sentimentUdf(col(\"TEXT\"))))\n",
    "    .select(col(\"sentiment\"))\n",
    "    .show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the series we have learned how to use more complex functions in the Snowflake dataframe API and how visualize datasets directly in a Jupyter notebook using the plotly-scala library. We also have learned how easy it is to enhance Snowflakes capabilities by creating Scala UDFs with third party libraries and arbitrary custom code. \n",
    "\n",
    "In the last installment we will combine all features we have seen far and will build a solution for an end-end Machine Learning usecase using the Weka ML library."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sL1Vq6r6J6dA",
    "zkvv6ruORQE9"
   ],
   "name": "ML_Training_and_Scoring_in_Scala_(Telco).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
