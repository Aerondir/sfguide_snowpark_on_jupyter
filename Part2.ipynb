{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHItaUbRB74u"
   },
   "source": [
    "![](jpg/stock_small.jpg)\n",
    "\n",
    "This is the second part of a multi-part series on Snowpark. In the first part I have reviewed a [Quick Start](#Quick-Start) guide, i.e. how to set up get started. The Quick Start steps will be exactly the same for this notebook. \n",
    "\n",
    "In this session, the focus will be\n",
    "\n",
    "- [Quick Start](#Quick-Start)\n",
    "- [Advanced API features and Visualization](#Advanced-API-features-and-Visualization)\n",
    "- [User Defined Functions](#User-Defined-Functions)\n",
    "\n",
    "All notebooks in this series require a Jupyter notebook environment with a Scala kernel. If you do not already have access to that type of environment I would higly recommend to use [Snowtire V2](https://github.com/zoharsan/snowtire_v2) and this excellent [post](https://medium.com/snowflake/from-zero-to-snowpark-in-5-minutes-72c5f8ec0b55). Additional instructions on versions uses in this series as well as how to make the notebook look nicer by using nbextensions can be found in this github repo.\n",
    "\n",
    "Versions used in this notebook are up-to-date as of August 2021. Please update them as necessary in the Snowtire setup step.\n",
    "\n",
    "In case you see any unexpected errors, restart the kernel via *Kernel -> Restart* and start the notebook from the beginning.\n",
    "\n",
    "Lastly, in my experience, when running on the Almond Kernel most problems reported by Ammonite REPL can be resolved by deleting the Ammonite cache and restarting the Kernel (as mentioned above).\n",
    "\n",
    "        docker exec -d SnowTrekPost rm -rf /home/jovyan/.cache/almond\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we have to set up the Environment for our Notebook. The instructions for setting up the environment are [here](https://docs.snowflake.com/en/developer-guide/snowpark/quickstart-jupyter.html#configuring-the-jupyter-notebook-for-snowpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 1\n",
    "\n",
    "Configure the notebook to use a Maven repository for a library that Snowpark depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:25:36.759106Z",
     "start_time": "2021-08-16T14:25:32.481Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val osgeoRepo = coursierapi.MavenRepository.of(\"https://repo.osgeo.org/repository/release\")\n",
    "interp.repositories() ++= Seq(osgeoRepo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 2\n",
    "\n",
    "Create a directory (if it doesn't exist) for temporary files created by the [REPL](https://ammonite.io/#Ammonite-REPL) environment. To avoid any side-effects from previous runs, we also delete any files that might exist in that directory.\n",
    "\n",
    "**Note: Make sure that you have the operating system permissions to create a directory in that location.**\n",
    "\n",
    "**Note: If you are using multiple notebooks, youâ€™ll need to create and configure a separate REPL class directory for each notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:25:37.781320Z",
     "start_time": "2021-08-16T14:25:32.482Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ammonite.ops._\n",
    "import ammonite.ops.ImplicitWd._\n",
    "\n",
    "// This folder is used to store generated repl classes, which will later be used in UDF.\n",
    "// Please provide an empty folder path.This is essential for Snowpark UDF to work\n",
    "val replClassPath = pwd+\"/repl_classes\"\n",
    "\n",
    "// delete any old files in the directory\n",
    "import sys.process._\n",
    "s\"rm -rf $replClassPath\" !\n",
    "\n",
    "// Create the repl class folder\n",
    "import sys.process._\n",
    "s\"mkdir -p $replClassPath\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 3\n",
    "\n",
    "Configure the compiler for the Scala REPL. This does the following:\n",
    "- Configures the compiler to generate classes for the REPL in the directory that you created earlier.\n",
    "- Configures the compiler to wrap code entered in the REPL in classes, rather than in objects.\n",
    "- Adds the directory that you created earlier as a dependency of the REPL interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:25:38.600364Z",
     "start_time": "2021-08-16T14:25:32.484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "// Generate all repl classes in repl class folder\n",
    "interp.configureCompiler(_.settings.outputDirs.setSingleOutput(replClassPath))\n",
    "interp.configureCompiler(_.settings.Yreplclassbased.value = true)\n",
    "interp.load.cp(os.Path(replClassPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 4\n",
    "Import the Snowpark library from Maven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:25:40.311971Z",
     "start_time": "2021-08-16T14:25:32.485Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`com.snowflake:snowpark:0.8.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's recreate 2 directories for later use, one for data files and one for user credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:25:40.960094Z",
     "start_time": "2021-08-16T14:25:32.487Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ammonite.ops._\n",
    "import ammonite.ops.ImplicitWd._\n",
    "\n",
    "%%('bash, \"-c\", \"[ -d data ] && echo 'data exists' || ln -s ~/snowtrek/data data\")\n",
    "%%('bash, \"-c\", \"[ -d creds ] && echo 'creds exists' || ln -s ~/snowtrek/creds creds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To create a session we need to authenticate ourselves to the Snowflake instance. Though it might be tempting to just override the authentication variables below with hard coded values, its not considered best practice to do so. In case you ever wanted to share your version of the notebook, your could disclose your credentials by mistake to the recipient. Even worse, if you upload your notebook to a public code repository, you might advertise your credentials to the whole wide world. To prevent that, you should keep your credentials in an external file (like we are doing here).\n",
    "\n",
    "Then update your credentials in that file and they will be save on your local machine. Even better if you do not use user/password authentication but [private key authentication](https://docs.snowflake.com/en/user-guide/key-pair-auth.html). \n",
    "\n",
    "Copy the credentials template file creds/template_credentials.txt to creds/credentials.txt and update the file with your credentials. Put your key files into the same directory or update the location accordingly in your credentials file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:25:47.800708Z",
     "start_time": "2021-08-16T14:25:32.489Z"
    },
    "hidden": true,
    "id": "kGGUvHHYjiGj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import com.snowflake.snowpark._\n",
    "import com.snowflake.snowpark.functions._\n",
    "\n",
    "val session = Session.builder.configFile(\"creds/credentials.txt\").create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 5\n",
    "Add the Ammonite kernel classes as dependencies for your UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:25:48.357198Z",
     "start_time": "2021-08-16T14:25:32.490Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def addClass(session: Session, className: String): String = {\n",
    "  var cls1 = Class.forName(className)\n",
    "  val resourceName = \"/\" + cls1.getName().replace(\".\", \"/\") + \".class\"\n",
    "  val url = cls1.getResource(resourceName)\n",
    "  val path = url.getPath().split(\":\").last.split(\"!\").head\n",
    "  session.addDependency(path)\n",
    "  path\n",
    "}\n",
    "addClass(session, \"ammonite.repl.ReplBridge$\")\n",
    "addClass(session, \"ammonite.interp.api.APIHolder\")\n",
    "addClass(session, \"pprint.TPrintColors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Step 6\n",
    "\n",
    "For this exercise we need three additional libraries\n",
    "- [plotl-scala (Plotly for Scala)](https://github.com/alexarchambault/plotly-scala)\n",
    "- [Spire (Numeric Abstractions for Scala)](https://typelevel.org/spire/)\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:25:50.174834Z",
     "start_time": "2021-08-16T14:25:32.491Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.8.2`\n",
    "import $ivy.`org.typelevel::spire:0.17.0`\n",
    "import $ivy.`edu.stanford.nlp:stanford-corenlp:4.2.2`\n",
    "\n",
    "import coursierapi._\n",
    "interp.load.ivy(\n",
    "      Dependency.of(\"edu.stanford.nlp\", \"stanford-corenlp\", \"4.2.2\").withClassifier(\"models\"),\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Advanced API features and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We completed the first part of this series using the Snowpark dataframe interface to project and filter datasets via the Snowpark databframe API. In the following section we will learn about more advanced features of the dataframe API like aggregations and pivot. Secondly, instead of just producing tabular result sets, we will visualize the results using a graphics package called [plotly](https://github.com/alexarchambault/plotly-scala). \n",
    "\n",
    "The for this section is to produce a dataframe that shows the count of open orders, filled orders by order date. The dataaframe should look similar to the matrix below\n",
    "\n",
    "    -------------------------------------------------\n",
    "    |\"O_ORDERDATE\"  |\"OPEN_COUNT\"  |\"FILLED_COUNT\"  |\n",
    "    -------------------------------------------------\n",
    "    |1996-06-23     |6242          |0               |\n",
    "    |1995-12-05     |6306          |0               |\n",
    "    |1995-07-18     |6236          |0               |\n",
    "    |1994-09-06     |0             |6285            |\n",
    "    |1992-04-22     |0             |6341            |\n",
    "    -------------------------------------------------\n",
    "\n",
    "We then want to visualize those counts via a line chart. \n",
    "\n",
    "Let's see how we can do that in Snowpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:26:57.437147Z",
     "start_time": "2021-08-16T14:26:56.286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val size:String=\"10\"\n",
    "val demoDataSchema:Seq[String]=Seq(\"SNOWFLAKE_SAMPLE_DATA\",\"TPCH_SF\"+size)\n",
    "val demoOrdersDf=session.table(demoDataSchema :+ \"ORDERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the next cell, we will filter the Orders by Status and only return Orders having the folling status values. \n",
    "\n",
    "    open (\"O\") \n",
    "    filled (\"F\") \n",
    "    \n",
    "We then select only the 3 columns we are interested in\n",
    "\n",
    "    O_ORDERDATE\n",
    "    O_ORDERSTATUS\n",
    "    O_ORDERKEY\n",
    "    \n",
    "Then we count the resulting rows by \n",
    "\n",
    "    date \n",
    "    status\n",
    "    \n",
    "This returns the count of Orders by Date and Status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:27:21.194755Z",
     "start_time": "2021-08-16T14:27:19.503Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val demoOrdersGroupedDf=\n",
    "        demoOrdersDf\n",
    "            .filter((col(\"O_ORDERSTATUS\")===lit(\"O\") || col(\"O_ORDERSTATUS\") === \"F\"))\n",
    "            .select(col(\"O_ORDERDATE\"),col(\"O_ORDERSTATUS\"),col(\"O_ORDERKEY\"))\n",
    "            .groupBy(col(\"O_ORDERDATE\"),col(\"O_ORDERSTATUS\"))\n",
    "            .agg(count_distinct(col(\"O_ORDERKEY\")).name(\"O_COUNT\"))\n",
    "\n",
    "demoOrdersGroupedDf.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, to visualize the dataset it would be more convenient to have one row per day with the count of open and filled orders. We accomplish that by using the pivot function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:27:36.051058Z",
     "start_time": "2021-08-16T14:27:35.115Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val demoOrdersGroupedPivotDf=\n",
    "        demoOrdersGrouped\n",
    "            .pivot(col(\"O_ORDERSTATUS\"),Seq('F','O'))\n",
    "            .sum(col(\"O_COUNT\"))\n",
    "            .select(col(\"O_ORDERDATE\"),coalesce(col(\"'O'\"),lit(0)).name(\"OPEN_COUNT\"),coalesce(col(\"'F'\"),lit(0)).name(\"FILLED_COUNT\"))\n",
    "\n",
    "demoOrdersGroupedPivotDf.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Viola! You hae created the data exactly as we designed it at the beginning. The last step is to visualize the data. \n",
    "\n",
    "In general, when we want to display a line chart for a single metric, we need 2 vectors of data. \n",
    "- The first vector is a Sequence of strings, also called Labels. \n",
    "- The second vector is a sequence of values, which have to be of data type double.\n",
    "If we have two metrics, we repeat the above structure of labels and values and wrap the two metrics in a sequence.\n",
    "\n",
    "So for our example we have the following structure:\n",
    "- Metric1: Open Orders\n",
    "  - Labels: Days\n",
    "  - Values: Open_Count\n",
    "- Metric2:\n",
    "  - Labels: Days\n",
    "  - Values: Filled_Count\n",
    "  \n",
    "As you can see, displaying the data is a breeze since we have prepared our data well. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:28:24.793720Z",
     "start_time": "2021-08-16T14:28:21.319Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "\n",
    "{\n",
    "    val data=demoOrdersGroupedPivot\n",
    "        .sort(col(\"O_ORDERDATE\"))\n",
    "        .collect()\n",
    "\n",
    "    val days=data.map(r => r(0).toString).toSeq\n",
    "    val open_count=data.map(r => r(1).toString.toDouble).toSeq\n",
    "    val filled_count=data.map(r => r(2).toString.toDouble).toSeq\n",
    "\n",
    "    plot(\n",
    "        Seq(\n",
    "          Scatter(days,open_count,name=\"OPEN_COUNT\")\n",
    "         ,Scatter(days,filled_count,name=\"FILLED_COUNT\"))\n",
    "         ,Layout(\n",
    "             title=\"Orders By Date\",\n",
    "             xaxis=Axis(title=\"Date\"),\n",
    "             yaxis=Axis(title=\"Orders per day\"),\n",
    "        )\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a more in depth discussions on the features of plotly, please review the [documentation](https://github.com/alexarchambault/plotly-scala). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# User Defined Funtions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Processing of Rational Numbers with Arbitrary Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the next section we will introduce another powerfull feature in Snowpark, i.e user defined functions also called UDFs. UDFs allow us to express arbitrary logic via Scala and execute that logic against massive datasets. The beauty is that the data doesn't have to move to the client machine but the necessary jar files will be deployed to Snowflake automatically by Snowpark. We just have to let Snowpark know how to resolve the dependencies. And we will learn how to do that in this section. \n",
    "\n",
    "Lets have a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example we will learn how to process Rational numbers with arbitrary precission. As you may know, Snowflake uses, besides other data types, double-precision (64 bit) IEEE 754 floating-point numbers. More about this topic can be found [here](https://docs.snowflake.com/en/sql-reference/data-types-numeric.html#float-float4-float8).\n",
    "\n",
    "So lets look at the following example:\n",
    "\n",
    "        12345678909876543219999-12345678909876543210000\n",
    "        \n",
    "In the following statement we have 2 numbers in string representation. We will cast those numbers to a floating point representation and subtract them from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:31:21.265900Z",
     "start_time": "2021-08-16T14:31:18.552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val longNumbers=session.createDataFrame(Seq((\"12345678909876543.9999\",\"12345678909876543.0000\"))).toDF(\"S1\",\"S2\")\n",
    "\n",
    "longNumbers\n",
    "    .withColumns(Seq(\"FP1\",\"FP2\"),Seq(callBuiltin(\"TO_DOUBLE\",col(\"S1\")),callBuiltin(\"TO_DOUBLE\",col(\"S2\"))))\n",
    "    .withColumn(\"RESULT\",col(\"FP1\")-col(\"FP2\"))\n",
    "    .select(col(\"FP1\"),col(\"FP2\"),col(\"RESULT\"))\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, instead of the expected 0.9999 we are getting 0. The reason is that (64 bit) IEEE 754 floating-point numbers can handle 16 digit long numbers, though they dont have enough significant digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So to avoid that problem we have to use numbers with an unbound precision. The OSS package [Spire](https://typelevel.org/spire/guide.html) addresses exactly that problem.\n",
    "\n",
    "In the following example I'll show how easy it is to take advantage of Spire to enjoy lossless processing of numbers with unbound precision. Note that we have already imported the package in [Step 6 of the Quick Start](#Step-6) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Creating UDF for Snowpark is straight forward. You can review the documentation [here](https://docs.snowflake.com/en/developer-guide/snowpark/creating-udfs.html#creating-udfs-in-jupyter-notebooks). Please note that the object / class needs to extend *Serializable*. For this example I am creating and object instead of a class since we need only one instance of the snowmath class (check [here](https://docs.scala-lang.org/tour/singleton-objects.html) for more details).\n",
    "\n",
    "The implemention of our object is very straight forward. We will use spire data type *Rational* and map the add/subtract/divide/multiply primitives to the corresponding spire functions. As a convenience function I have added a cast from Rational to Decimal just in case we wanted to see the value of a Rational number with a specific precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:32:21.039834Z",
     "start_time": "2021-08-16T14:32:20.245Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object snowmath extends Serializable {\n",
    "    \n",
    "    import spire.algebra._\n",
    "    import spire.math._\n",
    "    import spire.implicits._\n",
    "    import java.math.{MathContext, RoundingMode}  \n",
    "    \n",
    "    def opRational (op:String,r1:String,r2:String):String = {\n",
    "        op match {\n",
    "            case \"add\"      => (Rational(r1)+Rational(r2)).toString()\n",
    "            case \"subtract\" => (Rational(r1)-Rational(r2)).toString()\n",
    "            case \"divide\"   => (Rational(r1)/Rational(r2)).toString()\n",
    "            case \"multiply\" => (Rational(r1)*Rational(r2)).toString()\n",
    "            case _ => \"op not found\"\n",
    "        }\n",
    "    }\n",
    "    def fromRationalToBigDecimal(precision:String,r:String):String = {\n",
    "        Rational(r).toBigDecimal(precision.toInt,RoundingMode.HALF_EVEN).toString()\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rational Number in Spire are represented as n/d. This means, we have to change the first number to a corresponding expression\n",
    "\n",
    "    12345678909876543.9999 => 123456789098765439999/10000\n",
    "    \n",
    "And Voila, we get the expected result since spire can handle Rational numbers with unbound precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:33:37.104953Z",
     "start_time": "2021-08-16T14:33:36.627Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "snowmath.opRational(\"subtract\",\"123456789098765439999/10000\",\"12345678909876543\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, the cell above was running on our local machine. To execute the object in Snowflake we have to define the dependencies for our object above, and we have to create UDF mappings for the functions we want to call in Snowflake. Snowpark will then upload all necessary jar files to Snowflake and create the necessary mapping function for calling the scala object directly in SQL. This step could take some time depending on your ISP's upload speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:34:23.190965Z",
     "start_time": "2021-08-16T14:33:56.393Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "addClass(session,\"spire.math.Rational\")\n",
    "addClass(session,\"algebra.ring.Field\")\n",
    "addClass(session,\"cats.kernel.Order\")\n",
    "\n",
    "val opRationalUdf=session.udf.registerTemporary((op:String,fp1:String,fp2:String) => snowmath.opRational(op,fp1,fp2))\n",
    "val fromRationalToBigDecimalUdf=session.udf.registerTemporary((p:String,r:String) => snowmath.fromRationalToBigDecimal(p,r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To execute the opRationalUdf in a sql statement, \n",
    "\n",
    "- we create a Snowpark dataframe with our input numbers, \n",
    "- we create a new column which is the result from the computation\n",
    "- we create a another column casting the result to a decimal with 40 digit precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:36:23.765176Z",
     "start_time": "2021-08-16T14:36:17.727Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val longNumbersDf=session.createDataFrame(Seq((\"123456789098765439999/10000\",\"12345678909876543\"))).toDF(\"R1\",\"R2\")\n",
    "\n",
    "longNumbersDf\n",
    "    .withColumn(\"RESULT\",opRationalUdf(lit(\"subtract\"),col(\"R1\"),col(\"R2\")))\n",
    "    .withColumn(\"RESULT_DECIMAL\",fromRationalToBigDecimalUdf(lit(\"40\"),col(\"RESULT\")))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see above, snowmath can handle Rational numbers with unbound precision. The decimal result is still a string, however it could be cast to a floating point number in Snowflake. Just remember, that Snowflake floating point numbers have a maximum number of significant digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This example is similar to the previous though it shows a couple more features. \n",
    "\n",
    "- more complex UDF code\n",
    "- building a complex return object\n",
    "- lazy loading of data files\n",
    "\n",
    "The goal is to compute the sentiment of a text segment using the [Stanford CoreNLP library]((https://stanfordnlp.github.io/CoreNLP/) and return a JSON object that lists the sentence and the sentiment as follows:\n",
    "\n",
    "    [\n",
    "        {\n",
    "            \"sentence\": \"Happy Days\",\n",
    "            \"sentiment\": \"Positive\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "Note that we have already imported the package in Step 6 of the Quick Start ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T19:38:53.279365Z",
     "start_time": "2021-08-13T19:38:53.232Z"
    },
    "hidden": true
   },
   "source": [
    "Note the definition of *pipeline* and the fact, that *pipeline* is a class variable, i.e. its defined outside of the UDF *compute*. The initilization code for *pipeline* will be executed upon the first invocation of the Sentiment object. This is important to understand, since Snowflake allows a certain amount of time for class initialization (currently 300 secs) but only 5 secs for the method execution. So if *pipeline* was initialized within the scope of the *compute* method, it most likely would exceed the 5 second timeout and fail when executed in Snowflake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:38:17.018875Z",
     "start_time": "2021-08-16T14:38:16.337Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object Sentiment extends Serializable {\n",
    "\n",
    "    import java.io.File\n",
    "    import java.nio.charset.Charset\n",
    "    import java.util.Properties\n",
    "\n",
    "    import edu.stanford.nlp.coref.CorefCoreAnnotations\n",
    "    import edu.stanford.nlp.ling.CoreAnnotations\n",
    "    import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations\n",
    "    import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
    "    import edu.stanford.nlp.sentiment.SentimentCoreAnnotations\n",
    "    import edu.stanford.nlp.sentiment.SentimentCoreAnnotations.SentimentAnnotatedTree\n",
    "    import edu.stanford.nlp.util.CoreMap\n",
    "\n",
    "    import scala.collection.JavaConverters._\n",
    "    \n",
    "    import com.snowflake.snowpark.types._\n",
    "\n",
    "    private def getSentiment(sentiment: Int): String = sentiment match {\n",
    "        case x if x == 0 || x == 1 => \"Negative\"\n",
    "        case 2 => \"Neutral\"\n",
    "        case x if x == 3 || x == 4 => \"Positive\"\n",
    "    }\n",
    "\n",
    "    private val props: Properties = new Properties()\n",
    "    props.put(\"annotators\", \"tokenize, ssplit, parse, sentiment\")\n",
    " \n",
    "    private lazy val pipeline: StanfordCoreNLP = new StanfordCoreNLP(props)\n",
    "    \n",
    "    def compute (text:String):String = {\n",
    "\n",
    "        // create blank annotator\n",
    "        val document: Annotation = new Annotation(text)\n",
    "        \n",
    "        // run all Annotators\n",
    "        pipeline.annotate(document)\n",
    "\n",
    "        val sentences: List[CoreMap] = document.get(classOf[CoreAnnotations.SentencesAnnotation]).asScala.toList\n",
    "\n",
    "        \"[\\n\" + \n",
    "            sentences\n",
    "              .map(sentence => (sentence, sentence.get(classOf[SentimentAnnotatedTree])))\n",
    "              .map { case (sentence, tree) => \"{\\n\"+\n",
    "                                                \"  \\\"sentence\\\": \\\"\"+sentence.toString + \"\\\",\\n\" + \n",
    "                                                \"  \\\"sentiment\\\": \\\"\"+getSentiment(RNNCoreAnnotations.getPredictedClass(tree))+ \"\\\"\\n\" +\n",
    "                                               \"}\"\n",
    "                   }\n",
    "              .mkString(\",\") +\n",
    "        \"\\n]\"\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's test our Scala object locally with the text below.\n",
    "\n",
    "\"Mikail Farrar, a Georgia FedEx carrier who asked the universe for help and Tony Hawk responded. It's sweeter than it sounds. Farrar was on his usual route this week in the Atlanta suburbs when a 6-year-old boy chased him down and asked him to send a skateboard to Tony Hawk. He didn't have the half-pipe hero's address, so Farrar took a chance and tried to reach Hawk through TikTok. It worked! The social media-savvy skateboarder coordinated with Farrar to send the boy's skateboard to Hawk's correct address and send the 6-year-old a new board. It's just further proof that people can be wonderful and that Tony Hawk is as kind as he is gnarly, dude.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:38:38.251243Z",
     "start_time": "2021-08-16T14:38:30.911Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val text=\"\"\"\n",
    "Mikail Farrar, a Georgia FedEx carrier who asked the universe for help and Tony Hawk responded. It's sweeter than it sounds. Farrar was on his usual route this week in the Atlanta suburbs when a 6-year-old boy chased him down and asked him to send a skateboard to Tony Hawk. He didn't have the half-pipe hero's address, so Farrar took a chance and tried to reach Hawk through TikTok. It worked! The social media-savvy skateboarder coordinated with Farrar to send the boy's skateboard to Hawk's correct address and send the 6-year-old a new board. It's just further proof that people can be wonderful and that Tony Hawk is as kind as he is gnarly, dude.\n",
    "\"\"\"\n",
    "Sentiment.compute(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we have seen in the Spire example, all jar files (one of the classes implemented in the jar is just a proxy for the whole jar) have to be declared as dependencies. In this particular case, we also have a file storing the models. That file, i.e. the *Models* file will be listed separately, since it doesn't implement any classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:40:59.435750Z",
     "start_time": "2021-08-16T14:39:23.886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "addClass(session,\"edu.stanford.nlp.pipeline.StanfordCoreNLP\")\n",
    "addClass(session,\"org.ejml.simple.SimpleBase\")\n",
    "addClass(session,\"org.ejml.data.Matrix\")\n",
    "addClass(session,\"org.ejml.dense.row.CommonOps_DDRM\")\n",
    "\n",
    "session.addDependency(\"/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/edu/stanford/nlp/stanford-corenlp/4.2.2/stanford-corenlp-4.2.2-models.jar\")\n",
    "\n",
    "val sentimentUdf=udf((s:String) => Sentiment.compute(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T14:42:13.882806Z",
     "start_time": "2021-08-16T14:42:04.194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val sourceTextDf=session.createDataFrame(Seq(text)).toDF(\"TEXT\")\n",
    "\n",
    "sourceTextDf\n",
    "    .withColumn(\"sentiment\",callBuiltin(\"parse_json\",sentimentUdf(col(\"TEXT\"))))\n",
    "    .select(col(\"sentiment\"))\n",
    "    .show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the series we have learned how to visualize datasets directly in a Jupyter notebook using the plotly-scala library. We also have learned how easy it is to enhance Snowflakes capabilities by creating Scala UDFs with third party libraries and arbitrary custom code. In the last installment we will combine all features we have seen in the previous two notebooks and will build a solution for an end-end ML based usecase."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sL1Vq6r6J6dA",
    "zkvv6ruORQE9"
   ],
   "name": "ML_Training_and_Scoring_in_Scala_(Telco).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
